<img align="right" width="200" height="200" src="https://github.com/marcelglueck/BEpipeR/blob/e902703f223bb39e01afaa7ae16f511b60ef39ca/BEpipeR_logo.png">

# BEpipeR: a user-friendly, flexible, scalable, and easily expanded pipeline for a streamlined processing of biotic and abiotic data in R  
[![Github All Releases](https://img.shields.io/github/downloads/marcelglueck/BEpipeR/total.svg)]()
![GitHub Latest Release)](https://img.shields.io/github/v/release/marcelglueck/BEpipeR)
[![DOI](https://zenodo.org/badge/734299181.svg)](https://zenodo.org/doi/10.5281/zenodo.10683384)

## People involved
- [Marcel Gl√ºck](https://orcid.org/0000-0002-9027-6750) (PI, conceptualization, coding, implementation, maintenance, documentation)
- [Henri Thomassen](https://orcid.org/0000-0002-9403-1291) (conzeptualization)
- [Oliver Bossdorf](https://orcid.org/0000-0001-7504-6511) (conzeptualization)

## Updates
If you are interested in up-to-date news, you have come to the right place. Here we will
inform you about changes to the pipeline incl. future plans, newly-implemented features, 
and bugs/issues found and how to mitigate them.

### 20.02.2024
- **BEpipeR v0.1.0 has been released!** ü•≥ This release is already meant for productive purposes (i.e. it is fully functional with regards to forest plot data). We will fully implement support for grassland plots very soon. That release will also allow you to toggle between forest and grassland mode easily.

## Background and motivation
The wealth of biotic and abiotic environmental data generated in the [Biodiversity
Exploratories](https://www.biodiversity-exploratories.de/en/) continues to [grow steadily](https://www.bexis.uni-jena.de/ddm/publicsearch/), and so does the effort of implementing always the
newest data into our statistical frameworks. Unsurprisingly, many BE projects restrict their analyses to a
handful of frequently used data sets, neglecting the wealth of information at their fingertips.
Oftentimes, this might be caused by the need for stringent quality control and (pre-)processing
that many environmental data sets still require. However, this restraint might often prevent us
from obtaining a more complete understanding of our complex study systems. Hence, the aim
of this project is to establish a user-friendly, flexible, scalable, and easy-to-expand R
framework that permits the processing of (a)biotic data generated by the Exploratories.
We believe that such a framework will also benefit other scientists in the Exploratories, 
as the generated data can be used as input in any type of environmental association study.

This project is a registered BE synthesis project.

## Reasons to use the data generated by this pipeline
- **Save time**: No need to process the data sets yourself. Dive right into your analyses.
- **Harness potential**: Why restricting your analyses to a few often-used data sets when you could do better in unravelling complex patterns?
- **Flexibility**: You prefer subsetting for focal variables yourself? Great! Just use the quality-controlled complete composite and perform variables selection to your liking.
- **Improve participation:** By using one of the data sets produced here, you contribute to making the hard work of many BE scientists visible.

## The pipeline's main features
- **Easy to use**: All important parameters are parsed through xlsx/csv parameters files.
No coding required.
- **Easy to expand**: If you adhere to the input format of the parameters files, any type of
environmental information can be pre-processed, aggregated and post-processed
without writing a single line of code.
- **Easy to deploy**: The pipeline can be deployed on many systems without modifications
due to adhering to best practice approaches on reproducibility and interoperability.
- **Easy to debug and feature-expand**: Compressed code, the processing through a few major
loops, and consistent variables naming facilitate debugging and future feature implementations. In particular in the light of current high-paced discussions about good practice in data processing (e.g. [1](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003531) and [2](https://journals.asm.org/doi/full/10.1128/msphere.00355-23)), a flexible framework that can accommodate novel insights with little effort is more than timely.
- **Flexibility**: Regardless of whether you are interested in the complete data set, or
subsets produced by variance inflation factor analyses, we have you covered.
- **Speed**: Time-consuming steps are multi-threaded, reducing processing times.
- **Transparent and tractable**: The main parameters file is updated on-the-fly to reflect
the current state of processing, allowing the user to monitor the pipeline‚Äôs progress.
Also, column headers feature dataset IDs to allow users to select 
variables by their origin.

## What does the current version of the pipeline do?
1. **Data preparation and wrangling:** Template creation, plot locations harmonization, correction of suspicious (NA) values, sub-setting, fallbacks to more basal (taxonomic) levels, data reshaping, normalization by variable (for e.g. sampling effort)
2. **Quality control:** Multi-mode flagging and removal of putative erroneous values
3. **Data aggregation:** Both within and across data sets (mean, median, SD, MAD); processing of yearly climate aggregates (incl. the removal of poorly-supported data points)
4. **Diversity indices:** Normalization by repeated rarefaction; calculating species richness, Simpson/Shannon-Wiener/Margalef/Menhinick index, ...)
5. **Post-processing:** Data joining, quality control, variables selection by variance inflation factor (VIF) analyses with thresholds from two to ten
6. **Data export and metadata compilation:** Exporting whole composite and all VIF-produced subsets; fetching metadata to the variables produced to assist in preparing the data for publication, submission to Bexis, etc ...

## FAQ
### What to do if you found an undesired behaviour/bug in the pipeline?
First, please make sure that the potential issue you found is valid by inspecting the data produced by this section of the code carefully. If, after thorough inspection, you are still sure that you found an issue warranting correction, please bring this as soon as possible to the attention of [Marcel Gl√ºck](https://orcid.org/0000-0002-9027-6750) using the [bug report template](issues_template.txt).
### What to do if you find suspicious values in the data?
First, please ensure that the suspicious value found is not a false positive. The pipeline assists in finding and excluding suspicious values fairly extensively. Accordingly low should be their prevalence in the final data sets. If, after checking, you are still sure that you found such a value, please bring this to attention of Marcel Gl√ºck as soon as possible using the [bug report template](issues_template.txt). For erroneous information in metadata, an informal email to MG suffices.
### When will the pipeline be released to BE members?
We do not know yet. Currently, we are investigating possible licenses to prevent its un-attributed use. If progress is made in this regard, you will be informed on this page's 'Update' section and through the BE-intern mailing list.
### How to set up the pipeline on my system?
This pipeline uses the [renv package](https://rstudio.github.io/renv/articles/renv.html) to restore a reproducible environment we have set up. This means that the package installs all packages needed for an issue-free execution of the pipeline (i.e. the ones that were used to code this pipeline in the first place). For a step-by-step guide, please see [here](setup_guide.md). Installing all required packages like this is highly recommended and we will not try to troubleshoot potential issues caused by installing the required packages manually.
### Where can I find the manual?
We do not provide a comprehensive manual yet. The data already entered in the data sets summary file in combination with the file's dictionary should allow you to understand the encoding pretty intuitively. Also, in the script, we provide comments on the reasoning and each step performed throughout. If you want to adopt this framework for your own purposes by adding data sets, see this brief [schematic workflow](schematic_workflow.md). You also might appreciate this [explanation](output_description.md) of the output produced by this pipeline.
### Can I already use this pipeline to process grassland data without any modifications?
Unfortunately, not yet, but this feature will be implemented swiftly after the initial release of the pipeline. In the meantime, you can already start inspecting the grassland data sets you would like to include and enter this information in the data sets summary file. After we have implemented this feature, you will be able to toggle between grassland and forest mode easily.
### I disagree on statistical approaches chosen (e.g. normalization by repeated rarefaction). What should I do?
Nothing in biology is [black](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003531) or [white](https://journals.asm.org/doi/full/10.1128/msphere.00355-23). Throughout the pipeline, we provide the reasoning on the decisions taken. If you still disagree, the composite data set might be a good starting point for you. Go from there, subset, and perform variables selection to your liking. We are always open for constructive criticism, resulting in a more compelling processing of the data. Feel free to [spark a conversation](https://github.com/marcelglueck/BEpipeR/discussions).
### Why do I have to enter metadata information to the datasets summary file manually? Could this not be done by pulling this information from Bexis?
Indeed, most of the metadata might be pulled from Bexis without much of a struggle. However, from my experience, data and metadata sometimes do not match. By filling out these fields yourself, you are being forced to inspect the metadata, which contain important information, thoroughly. This should aid your understanding of the data set.
### I want to use this pipeline for aggregating quite some data sets from the Biodiversity Exploratories. What do I need to consider?
Keep in mind that if you are interested in working on more than a few data sets, for some reason, you need to have a synthesis proposal ... But do not panic. The corresponding Word template is only one page and your proposal does not undergo a formal approval process. Just state what the additional benefits of aggregating multiple data sets might be. You can find the template in Bexis.
### How do I attribute this pipeline?
This pipeline is citeable through its DOI. Please do so if you use the pipeline or parts of it in your own work. If you use data produced through this pipeline, please cite both the data set and this pipeline. If you share the code with someone else, please share the whole project, as separating any project file from the license is not permitted. For more information, see the pipeline's license file. 
### I like this project and want to contribute. How to go about it?
We appreciate your interest and are always happy to incorporate you and your unique skill set in this project. Please get in touch with [MG](https://orcid.org/0000-0002-9027-6750) for more information.

## Helpers
<p align="center">
‚ö†Ô∏èCurrently under construction...‚ö†Ô∏è
</p>

You would like to dive deeper into the composite data set. Nice! Here a few functions that 
might come in handy.

### Subsetting for variables originating from specific data sets
```R
# Specify your data sets of interest by BaseID and subset for composite columns
# identified based on partial matches
selected_data_sets <- c("14686", "14448", "19366")
selected_from_composite <-
  composite[, grep(paste(selected_data_sets, collapse = "|"),
                       names(composite),
                       value = TRUE), drop = FALSE]
```

## Acknowledgements
People and/or institutions we are indebted to:
- [Executive department on German Copyright Law](https://uni-tuebingen.de/en/facilities/university-library/publishing-research/publishing/copyright-law/), T√ºbingen University: For assisting in finding a suitable license for this pipeline.



