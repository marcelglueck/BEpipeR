# ------------------------------------------------------------------------------
# BEpipeR: a user-friendly, flexible, and scalable data synthesis pipeline for 
# the Biodiversity Exploratories and other research consortia
# ------------------------------------------------------------------------------
# Purpose: In combination with the three parameters files paramMAIN, paramDW and 
# paramSUB, this script processes (a)biotic EP-level Biodiversity Exploratories 
# data using one of three user-specifiable modes: i) forest: for the processing 
# of forest plots data, ii) grassland: for the processing of grassland plots data,
# and iii) combined: for the joint processing of both grassland and forest plots 
# data.

# Processing steps:
# 01. Import of plot IDs and location information; harmonization of plots' CRS
# 02. Data wrangling (e.g. correction of improperly used NA values)
# 03. Data sub-setting (e.g. exclusion of invalid/undesired factor levels)
# 04. Fall-backs to more basal (taxonomic) levels to resolve issues with species
#     aggregates
# 05. Data reshaping from long to wide format
# 06. Standardization by variable (for e.g. sampling effort)
# 07. Quality control I: Identification of undisclosed misused NA values and other
#     potential erroneous values by deviation from column-wise means and medians
# 08. Data set-intern aggregations
# 09. Group-intern aggregations
# 10. Processing of climate data obtained from BExIS climate tool
# 11. (Repeated) rarefaction of count data and calculation of alpha diversity 
#     indices
# 12. Left-joining of all processed data with plot IDs template
# 13. Quality control II: Identification of issues that occurred in compiling the
#     data; removal of mono-value/character columns and those that feature NA, NaN, 
#     Inf values; information harmonization for maximal downstream usability
# 14. Calculation of Pearson correlation coefficients and associated corrected 
#     P values for all variables pairs; variables selection by variance inflation 
#     factor (VIF) analyses
# 15. Export of the whole composite and VIF-produced subsets
# 16. Compilation and export of metadata from JSON datastructure files

# Usage remarks:
# - Execute this pipeline strictly sequentially. Otherwise, operations might be 
#   performed repeatedly, resulting in wrong values.
# - The pipeline's flow is mainly controlled through the parameters files. However,
#   user interventions might be required at some point. These points are marked
#   with "ACTION POTENTIALLY REQUIRED:". Currently, these are: i) selecting the 
#   BEpipeR mode to use, ii) setting the number-of-years-covered filter in filtering
#   plots in the processing of climate data, iii) avoiding the re-shaping to wide
#   format in DI for data sets already in this format, iv) excluding plots that would
#   make obtaining a continuous data set impossible (i.e. HEW51 after 2016), and v)
#   specifying the variables to retain in variables selection.
# - Know your data: This pipeline is agnostic to the biological meaningfulness of 
#   the user's data. It is in the user's responsibility to decide what data is
#   supplied.
# - For extensive information on abbreviations, consult paramMAIN's dictionary
# - Extensive data checking is performed. It is in the user's responsibility to 
#   check for warnings, errors, and other prompts generated by this pipeline.
# - Depending on the data sets processed, this pipeline might require more memory
#   than can be provided on 32-bit machines. Hence, make sure that you run 64-bit
#   R on a native 64-bit machine.

# Known issues:
# - If a column only contains 'T' or 'F' (e.g. 20040), the value is expanded to 
#   TRUE or FALSE when the file is exported. This is unlikely to be problematic,
#   as the forest/grassland column is never used in this pipeline. As the potential
#   solution 'quote = TRUE' is applied to the whole data set, it might cause issues
#   in downstream aggregation and, hence, this approach is not pursued.
# - usdm v2.1-6 does not allow for a VIF thresholds of one. This is most likely 
#   a bug that might be resolved in future releases.

# ------------------------------------------------------------------------------

# 1. RESTORING REPRODUCIBLE ENVIRONMENT
# Install packages from the existing per-project library:
# renv::restore()

# 2. LOADING PACKAGES
# Packages must be loaded like this. Otherwise, they will not be detected by renv.
# Do not change the order of the packages loaded, as this might result in breaking
# the pipeline by introducing masking issues!
library(usdm); library(plyr); library(here); library(tidyverse); library(data.table);
library(vegan); library(doSNOW); library(parallel); library(Hmisc); library(rtk)

# CUSTOM GLOBAL FUNCTIONS/OPTIONS
# Mute dplyr's grouping prompt and tidyverse's warnings about deprecated commands,
# as they can safely be ignored
options(dplyr.summarise.inform = FALSE)
options(lifecycle_verbosity = "quiet")

# For benchmarking, get total execution time
start_time <- Sys.time()

################################################################################
# MAIN BODY
################################################################################

# ------------------------------------------------------------------------------
# BEpipeR MODE SELECTION
# ------------------------------------------------------------------------------
# ACTION POTENTIALLY REQUIRED:
# Toggle here between forest, grassland and combined mode by specifying the
# respective string (forest, grassland, or combined). The BEpipeR mode specified
# does not (!) result in a premature sub-setting of input data. Instead, data
# sets containing both forest and grassland plots are kept as they are. They are  
# subset only shortly before the exclusion of columns with NA values in FQC (see
# below). This might result in slightly longer processing times, but preserves
# data, and hence potential, till the very end of the pipeline.
BEpipeR_mode <- "combined"

cat(paste0("Info: BEpipeR runs in **",
    BEpipeR_mode,
    "** mode. Abort if you would like to change this.\n"))
Sys.sleep(10)

# ------------------------------------------------------------------------------
# PREPARATION OF PLOT IDs TEMPLATE AND DATA SETS OVERVIEW TABLE;
# HARMONIZATION OF PLOTS' COORDINATE REFERENCE SYSTEMS (CRS)
# ------------------------------------------------------------------------------
# CREATE DIRECTORIES
dir.create(here("Output"), showWarnings = FALSE, recursive = FALSE)
dir.create(here("Processing"), showWarnings = FALSE, recursive = FALSE)
dir.create(here("Temp"), showWarnings = FALSE, recursive = FALSE)

# REMOVE OUTDATED FILES FROM THE PROCESSING AND OUTPUT DIRECTORIES
# Do not execute these lines before having loaded the 'here' package!
input_files <-
  list.files(here("Processing"), pattern = "\\.(txt|csv)$", full.names = TRUE)
output_files <-
  list.files(here("Output"), pattern = "\\.(png|txt|csv)$", full.names = TRUE)

if (length(input_files) > 0) {
  file.remove(input_files)
  cat("Info: Deleted", length(input_files), "outdated input files.\n")
  input_files <- c()
} else {
  cat("Info: No outdated input files found. No files deleted.\n")
}

if (length(output_files) > 0) {
  file.remove(output_files)
  cat("Info: Deleted", length(output_files), "outdated output files.\n")
  output_files <- c()
} else {
  cat("Info: No outdated output files found. No files deleted.\n")
}

# COPY ORIGINAL DATASETS FROM THE SOURCE TO THE PROCESSING DIRECTORY
# This allows for a fresh start of data processing with every new run of this
# pipeline
input_files_list <-
  list.files(here("Source"), "*.csv$", full.names = TRUE)
file_copy_report <- file.copy(input_files_list, here("Processing"), copy.date = TRUE)

if (all(file_copy_report)) {
  cat("Info: All input files were copied successfully to the Processing directory. You can continue safely.\n")
} else {
  cat("Warning: Some input files were not copied successfully to the Processing directory. Check your data.\n")
}

# Warn if data sets feature the same base ID. Because data sets are fetched by 
# pattern-matching for their base ID, having multiple data sets with the same
# base ID is not supported
input_files_baseIDs <-
  str_extract(
    list.files(
      here("Processing"),
      "^[[:digit:]]+_[[:digit:]]+_data.csv$",
      full.names = FALSE
    ),
    pattern = "^[[:digit:]]+"
  )

if (any(duplicated(input_files_baseIDs))) {
  input_files_baseIDs_dup <-
    input_files_baseIDs[duplicated(input_files_baseIDs)]
  cat(
    "Warning: The following data sets are duplicated. Solve this issue before you continue:" ,
    input_files_baseIDs_dup
  )
}

# IMPORT AND SUBSET LIST OF PLOTS USED FOR DOWNSTREAM DATA CONSOLIDATION
# Remove the grassland plots that were replaced soon after the BE's start in 2008;
# keep forest plot HEW02, as it was replaced in 2016, and hence, a lot of data is
# available for it.
setwd(here("Helpers"))
plotIDs_template <- read.csv(dir(pattern = paste0("^", 20826, "_")))
plotIDs_template <- plotIDs_template %>% 
  filter(!PlotID %in% c("A32886", "A8338"))
  
plotIDs_template <- plotIDs_template[, c("EP_PlotID", "PlotID", "RW", "HW")]

# Add leading zeroes to EP plot IDs
plotIDs_template$EPexplo <-
  str_extract(plotIDs_template$EP_PlotID, pattern = "^[[:letter:]]+")
plotIDs_template$EPplotnum <-
  str_pad(str_extract(plotIDs_template$EP_PlotID, pattern = "[[:digit:]]+$"),
    side = "left", width = 2, pad = "0")
plotIDs_template$EP_Plotid0 <-
  paste0(plotIDs_template$EPexplo, plotIDs_template$EPplotnum)

# Add leading zeroes to GP plot IDs
plotIDs_template$GPexplo <-
  str_extract(plotIDs_template$PlotID, pattern = "^[[:letter:]]+")
plotIDs_template$GPplotnum <-
  str_pad(str_extract(plotIDs_template$PlotID, pattern = "[[:digit:]]+$"),
    side = "left", width = 5, pad = "0")
plotIDs_template$GP_Plotid0 <-
  paste0(plotIDs_template$GPexplo, plotIDs_template$GPplotnum)

# Rename focal columns and subset for them
plotIDs_template <- plotIDs_template %>%
  rename("Longitude_DHDN" = "RW",
         "Latitude_DHDN" = "HW",
         "EP_Plotid" = "EP_PlotID",
         "GP_Plotid" = "PlotID")
plotIDs_template <-
  plotIDs_template[, c("EP_Plotid", "EP_Plotid0", "GP_Plotid", "GP_Plotid0",
                       "Longitude_DHDN", "Latitude_DHDN")]

# HARMONIZE PLOTS' CRS
# Plot locations from the different exploratories are provided in different CRS
# (see datastructure file of 11603). This prevents the obtaining of meaningful 
# inter-exploratories straight-line geographic distances. Still, straight-line 
# distances between plots within the same exploratory should be correct and very
# precise due to being located in their respective Gauss-Kruger zone. Hence, the
# rule of thumb is: For distances between plots of the same exploratory, use the
# DHDN data. For inter-exploratories distances, use the here constructed WGS84 
# (EPSG:4326) data.

# Import Germany's border for visualization; the terra package has already been 
# loaded as dependency
setwd(here("Helpers"))
GERM_border_WGS <-
  vect("Germany_borders.gpkg",
       layer = "ADM_ADM_1",
       crs = "EPSG:4326")

# Subset and convert plot location information to SpatObjects with original CRS 
# information embedded; store output in list
plots_ALB_noSpat <-
  plotIDs_template[grepl("^AE[WG]", plotIDs_template$EP_Plotid0), ]
plots_HAI_noSpat <-
  plotIDs_template[grepl("^HE[WG]", plotIDs_template$EP_Plotid0), ]
plots_SCH_noSpat <-
  plotIDs_template[grepl("^SE[WG]", plotIDs_template$EP_Plotid0), ]

plots_DHDN <- list(
  plots_ALB_Spat_DHDN <-
    vect(
      plots_ALB_noSpat,
      geom = c("Longitude_DHDN", "Latitude_DHDN"),
      crs = "EPSG:31467"
    ),
  plots_HAI_Spat_DHDN <-
    vect(
      plots_HAI_noSpat,
      geom = c("Longitude_DHDN", "Latitude_DHDN"),
      crs = "EPSG:31468"
    ),
  plots_SCH_Spat_DHDN <-
    vect(
      plots_SCH_noSpat,
      geom = c("Longitude_DHDN", "Latitude_DHDN"),
      crs = "EPSG:31469"
    )
)

# Re-project all DHDN plot location data to EPSG:4326 (WGS84)
plots_WGS <- list()
for (s in plots_DHDN) {
  plot_locs_reproj <- project(s, crs(GERM_border_WGS))
  plots_WGS <- c(plots_WGS, plot_locs_reproj)
}

# Extract and plot data
plots_ALB_Spat_WGS <- plots_WGS[[1]]
plots_HAI_Spat_WGS <- plots_WGS[[2]]
plots_SCH_Spat_WGS <- plots_WGS[[3]]
plot(GERM_border_WGS)
plot(plots_ALB_Spat_WGS, add = TRUE, col = "black", cex = 0.3)
plot(plots_HAI_Spat_WGS, add = TRUE, col = "blue", cex = 0.3)
plot(plots_SCH_Spat_WGS, add = TRUE, col = "brown", cex = 0.3)

# Add WGS84 location information to template
plots_WGS_all_df <- rbindlist(
  list(
    plots_ALB_Spat_WGS_df <-
      cbind(as.data.frame(plots_ALB_noSpat[, c("EP_Plotid")]), as.data.frame(geom(plots_ALB_Spat_WGS)[, c("x", "y")])),
    plots_HAI_Spat_WGS_df <-
      cbind(as.data.frame(plots_HAI_noSpat[, c("EP_Plotid")]), as.data.frame(geom(plots_HAI_Spat_WGS)[, c("x", "y")])),
    plots_SCH_Spat_WGS_df <-
      cbind(as.data.frame(plots_SCH_noSpat[, c("EP_Plotid")]), as.data.frame(geom(plots_SCH_Spat_WGS)[, c("x", "y")]))
  ),
  use.names = FALSE
)

names(plots_WGS_all_df)[1] <- "EP_Plotid"
plots_WGS_all_df <- plots_WGS_all_df %>%
  rename(Latitude_WGS84 = y, Longitude_WGS84 = x)

# Add WGS information to template
plotIDs_template <-
  left_join(plotIDs_template, plots_WGS_all_df, by = "EP_Plotid")

# Calculate distances in meter between plots to inspect the discrepancy between
# DHDN and WGS84. This must be done exploratory-wise for every DHDN. Do not change
# the order of entries in the two vectors below
explo_names_vec <- c("^AE[WG]", "^HE[WG]", "^SE[WG]")
explo_CRS_vec <- c("EPSG:31467", "EPSG:31468", "EPSG:31469")

distances_all_explos <- list()
for (a in 1:length(explo_names_vec)) {
  explo_name <- explo_names_vec[a]
  explo_name_clean <-  gsub("\\^", "", explo_name)
  explo_CRS <- explo_CRS_vec[a]
  explo_selected <-
    plotIDs_template[grepl(explo_name, plotIDs_template$EP_Plotid0),]
  distances_DHDN <-
    as.matrix(distance(vect(
      explo_selected,
      geom = c("Longitude_DHDN", "Latitude_DHDN"),
      crs = explo_CRS
    )))
  distances_WGS <-
    as.matrix(distance(vect(
      explo_selected,
      geom = c("Longitude_WGS84", "Latitude_WGS84"),
      crs = crs(GERM_border_WGS)
    )))
  # Calculate absolute distances in meter and export to list of matrices
  distances_diffs <- abs(distances_DHDN - distances_WGS)
  distances_all_explos[[explo_name_clean]] <- list(distances_diffs)
  # Give the user some more information on the 'error' introduced
  distances_max_diff <- round(max(distances_diffs), 2)
  distances_mean_diff <- round(mean(distances_diffs), 2)
  distances_median_diff <- round(median(distances_diffs), 2)
  distances_sd_diff <- round(sd(distances_diffs), 2)
  
  if (a == 1) {
    cat(
      "Info: Absolute reprojection-caused pairwise plots distance difference metrics:\n"
    )
  }
  cat(paste0(explo_name_clean, 
           ": mean: ", distances_mean_diff, ", ",
           "median: ", distances_median_diff, ", ",
           "SD: ", distances_sd_diff, ", ",
           "max: ", distances_max_diff, "m.\n"))
}

# Based on the BEpipeR mode specified, obtain vectors containing allowed GP and 
# EP plot designations for downstream sub-setting.
if (BEpipeR_mode == "forest") {
  plotIDs_forest <-
    plotIDs_template[grepl("^[AHS]EW", plotIDs_template$EP_Plotid0), ]
  plotIDs_allowed_EP <- c(plotIDs_forest$EP_Plotid, plotIDs_forest$EP_Plotid0)
  plotIDs_allowed_GP <- c(plotIDs_forest$GP_Plotid, plotIDs_forest$GP_Plotid0)
} else if (BEpipeR_mode == "grassland") {
  plotIDs_grassland <-
    plotIDs_template[grepl("^[AHS]EG", plotIDs_template$EP_Plotid0), ]
  plotIDs_allowed_EP <- c(plotIDs_grassland$EP_Plotid, plotIDs_grassland$EP_Plotid0)
  plotIDs_allowed_GP <- c(plotIDs_grassland$GP_Plotid, plotIDs_grassland$GP_Plotid0)
} else if (BEpipeR_mode == "combined") {
  plotIDs_allowed_EP <- c(plotIDs_template$EP_Plotid, plotIDs_template$EP_Plotid0)
  plotIDs_allowed_GP <- c(plotIDs_template$GP_Plotid, plotIDs_template$GP_Plotid0)
}

# Get mode-independent plot information for assigning ecosystems to plots regardless
# of plot designations used
plotIDs_allowed_forest <-
  unlist(plotIDs_template[grepl("^[AHS]EW", plotIDs_template$EP_Plotid0),][, c("GP_Plotid", "GP_Plotid0", "EP_Plotid", "EP_Plotid0")], use.names = FALSE)
plotIDs_allowed_grassland <-
  unlist(plotIDs_template[grepl("^[AHS]EG", plotIDs_template$EP_Plotid0),][, c("GP_Plotid", "GP_Plotid0", "EP_Plotid", "EP_Plotid0")], use.names = FALSE)

# Remove DHDN data from the plotIDs template, as this information does not allow
# for cross-exploratories comparison
plotIDs_template <- plotIDs_template %>% 
  select(-Longitude_DHDN, -Latitude_DHDN)

# Warn if the template contains NAs
if (any(is.na(plotIDs_template))) {
  cat("Warning: The template contains NAs which might result in incomplete data aggregation. Check your data.\n")
} else {
  cat("Info: No NAs were found in the template. You can continue safely.\n")
}

# IMPORT AND SUBSET OVERVIEW TABLE FOR IMPORTANT COLUMNS AND INCLUDED DATA SETS
# This table holds all important information to the data sets included in this 
# pipeline as well as the type of processing to be performed and keeps track 
# of all modifications (i.e. it is updated after each major modification)
datasets_table <-
  read.csv("paramMAIN.csv",
           header = TRUE,
           check.names = FALSE)
variables_columns <-
  as.vector(colnames(datasets_table[, grepl(pattern = "^C[[:digit:]]+$",
                                            colnames(datasets_table))]))
datasets_table <-
  datasets_table[, c(
    "BaseID", "Included", "Group", "QC", "rSUB", "rDW", "rSTD", "STDvar", 
    "rRES", "RESvar", "rFB", "FBlevel", "FBcol", "FBsub", "FBsep", "AsIt",
    "DIA", "DIAappr", "DIAcol1", "DIAcol2", "DIAcol3", "GIA", "GIAcol1", 
    "GIAcol2", "GIAcol3", "GIAabcorr", "GIAtaxcorr", "RF", "RFnrep", "DI", 
    variables_columns)]

datasets_table <-
  subset(datasets_table, datasets_table$Included == "yes")

# Warn if duplicated data sets are detected; this also indirectly prevents data
# sets from being assigned to multiple groups
if (any(duplicated(datasets_table$BaseID))) {
  cat(
    "Warning: Duplicated data sets in datasets_table detected. Resolve this issue before you continue.\n"
  )
}

# Warn if not all data sets flagged for processing are found in the Processing 
# directory
if (!(all(unique(datasets_table$BaseID) %in% input_files_baseIDs))) {
  cat(
    "Warning: Not all data sets listed in datasets_table appear to be in the Processing directory. Check your data.\n"
  )
}

# Get a copy of the prepared datasets_table for adding aggregation information to
# the metadata table constructed at the end of this pipeline. This streamlines
# completing the metadata by removing the need for going back and forth between 
# the paramMAIN and metadata file
metadata_info <- datasets_table

# Check the memory available and warn if presumably too low
gb_mem_avail <- round((memory.limit()) / 1024, 1)
cat("Info: Checking total system-wide memory available ...\n")
if (gb_mem_avail < 12) {
  cat(
    "Warning:",
    gb_mem_avail,
    "GB is available. This might not be sufficient for executing this pipeline.\n"
  )
} else {
  cat("Info: In total,", gb_mem_avail, "GB of memory is available.\n")
}

# ------------------------------------------------------------------------------
# IDENTIFY DATA SETS/VARIABLES THAT REQUIRE DATA WRANGLING/SUBSETTING/RESHAPING/
# STANDARDIZATION/FALLBACKS
# ------------------------------------------------------------------------------
# Data sets requiring data wrangling
DW <- subset(datasets_table, datasets_table$rDW == "yes")

# Data sets requiring sub-setting
SUB <- subset(datasets_table, datasets_table$rSUB == "yes")

# Data sets requiring reshaping from long to wide format
# (data sets that do not need to be immediately re-shaped (e.g. abundance data)
# are later on-the-fly re-shaped)
RES <- subset(datasets_table, datasets_table$rRES == "yes")

# Variables requiring standardization by variable
STD <- subset(datasets_table, datasets_table$rSTD == "yes")

# Data sets requiring fallbacks
FB <- subset(datasets_table, datasets_table$rFB == "yes")

# ------------------------------------------------------------------------------
# DATA WRANGLING (DW): CORRECT IMPROPER NA VALUES AND OTHER ERRONEOUS ENTRIES
# ------------------------------------------------------------------------------
# The data wrangling helper (paramDW) supports multiple entries for the same data
# set. This means that the same data set can be modified with different approaches
# that are applied consecutively. This also means that the order of requested 
# modifications in the helper does matter (i.e. modifications are applied from 
# top to bottom). Currently supported is the exact replacement of numbers/strings/NA
# values with numbers/NA values. Also, for a user-defined sub-string, complete 
# rows can be deleted or replacements can be made via a pattern-matching approach.

if (nrow(DW) > 0) {
  # Import helper and check whether it lists all data sets flagged for data wrangling
  setwd(here("Helpers"))
  DW_helper <- read.csv("paramDW.csv", header = TRUE)
  
  if (identical(sort(unique(DW$BaseID)), sort(unique(DW_helper$Dataset_ID)))) {
    cat(
      "Info: All data sets flagged for data wrangling are listed in paramDW. You can continue safely.\n"
    )
  } else {
    cat(
      "Warning: There is a discrepancy between the data sets flagged for data wrangling and the ones listed in paramDW. Check your data.\n"
    )
  }
  
  # PERFORM ALL REQUESTED MODIFICATIONS WHILE DISCRIMINATING BETWEEN EXACT (VALUE)
  # AND PATTERN-BASED APPROACHES
  cat("Info: Performing data wrangling ...\n")
  setwd(here("Processing"))
  for (t in unique(DW_helper$Dataset_ID)) {
    # LOAD SELECTED DATA SET
    # Prompt about the pipeline's progress
    cat("Info: Processing data set", t, "...\n")
    DW_helper_entry <- subset(DW_helper, DW_helper$Dataset_ID == t)
    DW_selected_ds <-
      read.csv(dir(pattern = paste0("^", t, "_")), check.names = FALSE)
    
    for (g in 1:nrow(DW_helper_entry)) {
      DW_helper_subentry <- DW_helper_entry[g, ]
      
      # PERFORM VALUE-BASED (EXACT) APPROACHES
      if (DW_helper_subentry$Class == "value") {
        DW_val_old <- DW_helper_subentry$Value_old
        DW_val_new <- DW_helper_subentry$Value_new
        
        # Adjust the replacement value to numeric if needed
        if (!is.na(DW_val_new) && !is.null(DW_val_new)) {
          DW_val_new <- as.numeric(DW_val_new)
        }
        
        # If present, replace NAs using a different approach; if not present,
        # perform a simple replacement
        if (is.na(DW_val_old)) {
          DW_selected_ds <- DW_selected_ds %>% replace(is.na(.), DW_val_new)
        } else {
          DW_selected_ds[DW_selected_ds == DW_val_old] <- DW_val_new
        }
      }
      
      # PERFORM PATTERN-BASED APPROACHES
      if (DW_helper_subentry$Class == "pattern") {
        DW_val_old <- DW_helper_subentry$Value_old
        DW_val_new <- DW_helper_subentry$Value_new
        
        # Perform pattern-based row deletions
        if (DW_val_new == "NULL") {
          DW_flagged_rows <- numeric(0)
          for (col in 1:ncol(DW_selected_ds)) {
            DW_matching_rows <- grepl(DW_val_old, DW_selected_ds[, col])
            DW_flagged_rows <-
              union(DW_flagged_rows, which(DW_matching_rows))
          }
          DW_selected_ds <- DW_selected_ds[-DW_flagged_rows,]
          # Expunge variable to be filled for the next sub-entry
          DW_flagged_rows <- numeric(0)
        } else {
          # Perform pattern-based replacements
          for (col2 in 1:ncol(DW_selected_ds)) {
            DW_matching_cells <- grepl(DW_val_old, DW_selected_ds[, col2])
            DW_selected_ds[DW_matching_cells] <- DW_val_new
          }
        }
      }
    }
    
    # UPDATE DATASETS_TABLE
    # Get the corresponding entry in datasets_table
    DW_datasets_table_modified_entry <-
      subset(datasets_table, datasets_table$BaseID == t)
    # Delete the entry in datasets_table
    datasets_table <- datasets_table[!datasets_table$BaseID == t,]
    # Clear the 'rDW=yes' flag
    DW_datasets_table_modified_entry$rDW <- "no"
    # Add the updated entry to datasets_table
    datasets_table <-
      rbind(datasets_table, DW_datasets_table_modified_entry)
    
    # EXPORT MODIFIED DATA SET
    DW_original_ds <-
      list.files(getwd(),
                 pattern = paste0("^", t),
                 full.names = TRUE)
    # Overwrite original data set with the newly-generated one
    write.csv(DW_selected_ds, DW_original_ds, row.names = FALSE)
  }
} else {
  cat("Info: No data sets flagged for data wrangling. Skipping ...\n")
}

# ------------------------------------------------------------------------------
# SUBSET (SUB) DATA SETS 
# ------------------------------------------------------------------------------
# Sub-setting removes certain unwanted information. As for data wrangling, multiple 
# subset steps that are applied consecutively can be requested for the same data set. 
# Supported are all comparison operators.

if (nrow(SUB) > 0) {
  # Load paramSUB and compare its entries with datasets_table to identify modifications
  # requested but not performed
  setwd(here("Helpers"))
  SUB_helper <-
    read.csv(file = "paramSUB.csv",
             header = TRUE,
             check.names = FALSE)
  
  if (identical(sort(unique(SUB$BaseID)), sort(unique(SUB_helper$Dataset_ID)))) {
    cat(
      "Info: All data sets flagged for sub-setting are listed in paramSUB. You can continue safely.\n"
    )
  } else {
    cat(
      "Warning: There is a discrepancy between the data sets flagged for sub-setting and the ones listed in paramSUB. Check your data.\n"
    )
  }
  
  cat("Info: Performing sub-setting ...\n")
  setwd(here("Processing"))
  for (x in unique(SUB_helper$Dataset_ID)) {
    # IMPORT DATA SET
    # Prompt about the pipeline's progress
    cat("Info: Processing data set", x, "...\n")
    SUB_selected_ds <-
      read.csv(dir(pattern = paste0("^", x, "_")), check.names = FALSE)
    
    # Get all modifications requested for the selected data set
    SUB_helper_entry <- subset(SUB_helper, SUB_helper$Dataset_ID == x)
    
    # APPLY ALL SUBSETTINGS REQUESTED FOR THE DATA SET
    for (z in 1:nrow(SUB_helper_entry)) {
      # Get all required information from paramSUB
      SUB_metadata <- SUB_helper_entry[z,]
      SUB_variable <- as.character(SUB_metadata$Subset_variable)
      SUB_operator <- as.character(SUB_metadata$Operator)
      SUB_level <- as.character(SUB_metadata$Subset_level)
      
      # Subset the data set using the operator requested
      SUB_filter_condition <-
        paste0(SUB_variable, SUB_operator, "'", SUB_level, "'")
      # Overwrite the existing data set to combine multiple sub-settings
      SUB_selected_ds <-
        SUB_selected_ds %>% filter(eval(parse(text = SUB_filter_condition)))
    }
    
    # UPDATE OVERVIEW TABLE
    # (Updating focal variables is not needed, as no variables were deleted
    # or modified)
    # Get the corresponding entry in datasets_table
    SUB_datasets_table_modified_entry <-
      subset(datasets_table, datasets_table$BaseID == x)
    # Delete the outdated entry in datasets_table
    datasets_table <- datasets_table[!datasets_table$BaseID == x,]
    # Update the subsetting information of the entry selected
    SUB_datasets_table_modified_entry$rSUB <- "no"
    # Append the updated entry to datasets_table
    datasets_table <-
      rbind(datasets_table, SUB_datasets_table_modified_entry)
    
    # EXPORT SUBSET DATA SETS
    # Get original file name
    SUB_original_ds <-
      list.files(getwd(),
                 pattern = paste0("^", x),
                 full.names = TRUE)
    # Overwrite original data set with the newly-generated one
    write.csv(SUB_selected_ds, SUB_original_ds, row.names = FALSE)
  }
} else {
  cat("Info: No data sets flagged for subsetting. Skipping ...\n")
}

# ------------------------------------------------------------------------------
# PERFORM FALLBACKS (FB) TO MORE BASAL (TAXONOMIC) LEVELS 
# ------------------------------------------------------------------------------
# After having excluded undesired variables and factor levels in DW and SUB, 
# fallbacks to more basal (taxonomic) levels can safely be performed. This should 
# be done before re-shaping, because collapsing different variables is less straight-
# forward in wide than in long format. Always use sums to aggregate the data. 
# Currently, fallbacks to multiple alternative levels are not supported.

if (nrow(FB) > 0) {
  cat("Info: Performing fallbacks to more basal (taxonomic) levels ...\n")
  setwd(here("Processing"))
  for (o in FB$BaseID) {
    # Prompt about the pipeline's progress
    cat("Info: Processing data set", o, "...\n")
    # Open selected data set
    FB_selected_ds <-
      read.csv(dir(pattern = paste0("^", o, "_")), check.names = FALSE)
    # Get info required for performing the fallback from datasets_table
    FB_metadata <-
      subset(datasets_table, datasets_table$BaseID == o)
    FB_col <- as.character(FB_metadata$FBcol)
    FB_sub <- as.integer(FB_metadata$FBsub)
    FB_sep <- as.character(FB_metadata$FBsep)
    FB_plot <- as.character(FB_metadata$C1)
    # Get focal variables and subset for them
    FB_focal_variables <-
      FB_metadata[, (which(names(FB_metadata) == "C1")):ncol(FB_metadata)]
    FB_focal_variables <-
      unlist(FB_focal_variables %>% select_if( ~ !all(is.na(.x) |
                                                        .x == "")))
    FB_selected_ds <- FB_selected_ds[, FB_focal_variables]
    
    # PERFORM FALLBACK
    FB_selected_ds[[FB_col]] <-
      sapply(strsplit(as.character(FB_selected_ds[[FB_col]]), FB_sep), function(x)
        x[FB_sub])
    FB_selected_ds <- FB_selected_ds %>%
      group_by_at(vars(FB_plot, FB_col)) %>%
      summarise(across(where(is.numeric), sum))
    
    # UPDATE DATASETS TABLE
    # Delete the outdated entry in datasets_table
    datasets_table <- datasets_table[!datasets_table$BaseID == o, ]
    # Update the selected metadata entry to reflect the processing
    FB_metadata$rFB <- "no"
    FB_metadata$FBlevel <- ""
    FB_metadata$FBcol <- ""
    FB_metadata$FBsub <- ""
    FB_metadata$FBsep <- ""
    # Append updated entry to datasets_table
    datasets_table <- rbind(datasets_table, FB_metadata)
    
    # EXPORT MODIFIED DATA SET
    # Get original file name
    FB_original_ds <-
      list.files(getwd(),
                 pattern = paste0("^", o),
                 full.names = TRUE)
    # Overwrite original data set with the newly-generated one
    write.csv(FB_selected_ds, FB_original_ds, row.names = FALSE)
  }
} else {
  cat("Info: No data sets flagged for fall-backs. Skipping ...\n")
}

# ------------------------------------------------------------------------------
# RESHAPE (RES) SELECTED DATA SETS
# ------------------------------------------------------------------------------
# This step converts selected data sets from long to wide format. Reshaping is 
# purposely performed after DW, SUB and FB as these operations are less straight-
# forward in wide format. In reshaping, plot-wise means are calculated to collapse
# duplicated entries. Missing combinations are coded as NAs (and not (!) as zeros).
# Important: Do not reshape data sets here that will be calculated diversity indices 
# later on, as they are required in long format and the replacement value here is 
# incorrect for these data.

if (nrow(RES) > 0) {
  # For a straightforward downstream removal of RES suffixes from variables names,
  # collect them in a vector
  RES_suffixes_all <- c()
  
  cat("Info: Reshaping selected data sets while calculating plot-wise means ...\n")
  setwd(here("Processing"))
  for (i in RES$BaseID) {
    # Prompt about the pipeline's progress
    cat("Info: Processing data set", i, "...\n")
    # Open selected data set
    RES_selected_ds <-
      read.csv(dir(pattern = paste0("^", i, "_")), check.names = FALSE)
    # Identify focal and reshaping variables for the data set loaded
    RES_focal_variables <-
      subset(datasets_table, datasets_table$BaseID == i)
    RESvar <- RES_focal_variables$RESvar
    RES_focal_variables <-
      RES_focal_variables[, (which(names(RES_focal_variables) == "C1")):ncol(RES_focal_variables)]
    RES_focal_variables <-
      RES_focal_variables %>% select_if(~ !all(is.na(.x) |
                                                 .x == ""))
    RES_focal_variables <- c(RES_focal_variables[1,])
    # Subset selected data set for focal variables
    RES_selected_ds <- RES_selected_ds[unlist(RES_focal_variables)]
    # Clear potential leftover 'none' entries that should have been removed in the
    # SUB section)
    RES_selected_ds[RES_selected_ds == "none"] <- NA
    # Adjust variables types
    RES_selected_ds <- RES_selected_ds %>%
      mutate_at(vars(3:ncol(RES_selected_ds)), as.numeric)
    RES_selected_ds <- RES_selected_ds %>%
      mutate_at(vars(1:2), as.factor)
    
    # Calculate plot-wise means for each level of RESvar to solve problems caused
    # by duplicated entries
    RES_selected_ds_aggr <- RES_selected_ds %>%
      group_by_at(vars(RES_focal_variables[[1]], RES_focal_variables[[2]])) %>%
      summarise(across(where(is.numeric), mean))
    
    # Reshape data set from long to wide format
    RES_selected_ds_aggr_wide <- RES_selected_ds_aggr %>%
      pivot_wider(
        names_from = RES_focal_variables[[2]],
        values_from = names(RES_selected_ds_aggr)[3]:names(RES_selected_ds_aggr)[ncol(RES_selected_ds_aggr)]
      )
    
    # Extract and append suffix information to vector
    RES_suffixes <-
      paste0("_", as.character(unique(RES_selected_ds_aggr[[RES_focal_variables[[2]]]])))
    RES_suffixes_all <- c(RES_suffixes_all, RES_suffixes)
    
    # Delete potentially introduced NA columns
    if ("NA" %in% colnames(RES_selected_ds_aggr_wide)) {
      RES_selected_ds_aggr_wide <-
        select(RES_selected_ds_aggr_wide, -c("NA"))
    }
    
    # UPDATE OVERVIEW TABLE
    # Get the corresponding entry from datasets_table
    RES_datasets_table_modified_entry <-
      subset(datasets_table, datasets_table$BaseID == i)
    # Delete the outdated entry in datasets_table
    datasets_table <- datasets_table[!datasets_table$BaseID == i, ]
    # Update the processing information of the entry selected
    RES_datasets_table_modified_entry$rRES <- "no"
    RES_datasets_table_modified_entry$RESvar <- ""
    # Get the names of the variables just calculated and delete the outdated ones
    RES_produced_variables <- names(RES_selected_ds_aggr_wide)
    RES_old_variables_start <-
      which(colnames(RES_datasets_table_modified_entry) == "C1")
    RES_datasets_table_modified_entry[, RES_old_variables_start:ncol(RES_datasets_table_modified_entry)] <-
      ""
    # Add the names of the newly-generated variables
    RES_datasets_table_modified_entry[1, RES_old_variables_start:(RES_old_variables_start + length(RES_produced_variables) - 1)] <-
      RES_produced_variables
    # Add the updated entry to datasets_table
    datasets_table <-
      rbind(datasets_table, RES_datasets_table_modified_entry)
    
    # EXPORT MODIFIED DATA SET
    # Get original file name
    RES_original_ds <-
      list.files(getwd(),
                 pattern = paste0("^", i),
                 full.names = TRUE)
    # Overwrite original data set with the newly-generated one
    write.csv(RES_selected_ds_aggr_wide, RES_original_ds, row.names = FALSE)
  }
} else {
  cat("Info: No data sets flagged for reshaping. Skipping ...\n")
}

# ------------------------------------------------------------------------------
# STANDARDIZE (STD) VARIABLE(S) BY VARIABLE
# ------------------------------------------------------------------------------
# Sometimes, non-standardized variables are provided that require non-rarefaction- 
# based standardization for sampling effort to render the metrics comparable between
# plots. In this case, standardization for sampling effort is performed using a
# variable provided in the data set. Please note: The terms standardization/
# normalization are used as synonyms.

if (nrow(STD) > 0) {
  cat("Info: Performing standardization/normalization by variable ...\n")
  for (i in STD$BaseID) {
    # Prompt about the pipeline's progress
    cat("Info: Processing data set", i, "...\n")
    # Open selected data set
    STD_selected_ds <-
      read.csv(dir(pattern = paste0("^", i, "_")), check.names = FALSE)
    # PERFORM STANDARDIZATION
    STD_old_variables_start <- which(colnames(datasets_table) == "C1")
    STD_focal_variables <-
      datasets_table[datasets_table$BaseID == i, STD_old_variables_start:ncol(datasets_table)]
    STD_focal_variables <-
      as.vector(unique(STD_focal_variables[STD_focal_variables != "" &
                                             !is.na(STD_focal_variables)]))
    STD_stdvar <-
      as.vector(datasets_table[datasets_table$BaseID == i, "STDvar"])
    STD_vars_all <- c(STD_focal_variables, STD_stdvar)
    STD_selected_ds <- STD_selected_ds %>% select(STD_vars_all)
    STD_target_variables <-
      STD_vars_all[2:length(STD_vars_all)]
    # Perform standardization and check whether following it, the variable used for
    # standardization is all one
    STD_selected_ds[STD_target_variables] <-
      STD_selected_ds[STD_target_variables] / STD_selected_ds[[STD_stdvar]]
    
    if (any(STD_selected_ds[STD_stdvar] != 1)) {
      cat(
        paste0(
          "Warning: Following standardization, STDvar has values unequal to one. This hints towards unsuccessful standardization. Check data set ",
          i,
          ".\n"
        )
      )
    }
    
    # Delete the column holding the variable used for standardization
    STD_selected_ds <- select(STD_selected_ds, -c(STD_stdvar))
    
    # Get original file name
    STD_original_ds <-
      list.files(getwd(),
                 pattern = paste0("^", i),
                 full.names = TRUE)
    # Overwrite original data set with the newly-generated one
    write.csv(STD_selected_ds, STD_original_ds, row.names = FALSE)
    
    # UPDATE OVERVIEW TABLE
    # Get the corresponding entry in datasets_table
    STD_datasets_table_modified_entry <-
      subset(datasets_table, datasets_table$BaseID == i)
    # Delete the corresponding entry in datasets_table
    datasets_table <- datasets_table[!datasets_table$BaseID == i, ]
    # Update the processing information of the entry selected; updating focal
    # variables is not required, as no variables were added or removed
    STD_datasets_table_modified_entry$rSTD <- "no"
    STD_datasets_table_modified_entry$STDvar <- ""
    # Add the updated entry to datasets_table
    datasets_table <-
      rbind(datasets_table, STD_datasets_table_modified_entry)
  }
} else {
  cat("Info: No data sets flagged for standardization by variable. Skipping ...\n")
}

# ------------------------------------------------------------------------------
# DATA QUALITY CONTROL (QC)
# ------------------------------------------------------------------------------
# This step is to find potential invalid variables states hinting towards improper
# (NA) values not disclosed in the data sets' metadata, as well as species aggre-
# gates that result in groups with artificially high or low abundance scores. Both
# deviations from mean and median are checked. This step complements manual data
# exploration, which it cannot replace: 'Know your data!' still holds. All included
# data sets except for climate data are checked, as the latter has been extensively 
# quality-checked by BExIS team. If BEpipeR runs in non-combined mode, plots not 
# in concordance with the mode specified are excluded, as they might trigger false
# alarms. If BEpipeR runs in combined mode, outlier detection is performed on both
# forest and grassland data separately. Data sets can be excluded from QC by speci-
# fying QC = "no" in paramMAIN.

QC_ds_list <- subset(datasets_table, datasets_table$QC == "yes" & 
                       datasets_table$BaseID != "19007")

if (nrow(QC_ds_list) > 0) {
  QC_outside_CI_mean_list <- list()
  QC_outside_CI_median_list <- list()
  
  # For constructing confidence intervals, specify the accepted amount of variance
  # (quantified as multiple of SD) here:
  QC_acc_var_SD <- 18

  setwd(here("Processing"))
  cat("Info: Performing quality control ...\n")
  for (u in QC_ds_list$BaseID) {
    # Prompt about the pipeline's progress
    cat("Info: Processing data set", u, "...\n")
    # LOAD THE SELECTED DATA SET AND SUBSET FOR FOCAL VARIABLES
    QC_selected_ds <-
      read.csv(dir(pattern = paste0("^", u, "_")), check.names = FALSE)
    QC_start_focal_variables <-
      which(colnames(QC_ds_list) == "C1")
    QC_selected_entry <- subset(QC_ds_list, QC_ds_list$BaseID == u)
    QC_focal_variables <-
      QC_selected_entry[, QC_start_focal_variables:ncol(QC_selected_entry)]
    QC_focal_variables <-
      QC_focal_variables %>% select_if( ~ !all(is.na(.x) | .x == ""))
    QC_selected_ds <-
      QC_selected_ds[, as.character(QC_focal_variables)]
    
    # Force all but the first column to as.numeric while suppressing warnings
    # about the introduction of NAs by coercing strings to numeric
    suppressWarnings(QC_selected_ds <- QC_selected_ds %>%
                       mutate_at(c(2:ncol(QC_selected_ds)), as.numeric))
    
    # Add helper column that includes ecosystem information
    QC_selected_ds$QC_ecosys <-
      ifelse(
        QC_selected_ds[, 1] %in% plotIDs_allowed_forest,
        "forest",
        ifelse(
          QC_selected_ds[, 1] %in% plotIDs_allowed_grassland,
          "grassland",
          ""
        )
      )
    
    # Exclude undesired plots with respect to the BEpipeR mode specified to prevent
    # false alarms. If BEpipeR runs in combined mode, no sub-setting is performed,
    # but outlier detection is performed for both ecosystems separately.
    if (BEpipeR_mode == "forest") {
      QC_selected_ds <-
        subset(QC_selected_ds, QC_selected_ds$QC_ecosys != "grassland")
    } else if (BEpipeR_mode == "grassland") {
      QC_selected_ds <-
        subset(QC_selected_ds, QC_selected_ds$QC_ecosys != "forest")
    }
    
    # Warn about special values (Inf, NaN)
    if (any(is.infinite(as.matrix(QC_selected_ds)) |
            is.nan(as.matrix(QC_selected_ds)))) {
      cat("Warning:",
          u,
          "contains infinite or NaN values. Please check this data set.\n")
    }
    
    # Perform ecosystem-wise outlier detection
    for (p in unique(QC_selected_ds$QC_ecosys)) {
      # Subset for the ecosystem selected
      QC_selected_ecosys_ds <-
        subset(QC_selected_ds, QC_selected_ds$QC_ecosys == p)
      
      # Drop columns that contain less than two valid values and remove the helper
      # column holding ecosystem information
      QC_selected_ecosys_ds <- QC_selected_ecosys_ds %>%
        select_if(~ sum(!is.na(.)) >= 2) %>%
        select(-QC_ecosys)
      
      # CALCULATE COLUMN-WISE MEANS AND SD-BASED CONFIDENCE INTERVALS
      QC_col_MEAN <- QC_selected_ecosys_ds %>% select_if(is.numeric) %>%
        summarise_all( ~ mean(., na.rm = TRUE))
      QC_col_SD <- QC_selected_ecosys_ds %>% select_if(is.numeric) %>%
        summarise_all( ~ sd(., na.rm = TRUE))
      QC_col_MEAN_upper_bound <-
        QC_col_MEAN + (QC_acc_var_SD * QC_col_SD)
      QC_col_MEAN_lower_bound <-
        QC_col_MEAN - (QC_acc_var_SD * QC_col_SD)
      # As negative lower bounds might be unreasonable for most data, the values
      # are adjusted to zero if the column-wise mean is non-negative but the
      # lower bound is. If the lower bound is positive, it is kept unchanged.
      # This approach might slightly increase the probability of false positives
      # but is nonetheless preferred to missing potential problematic values.
      for (i in 1:length(QC_col_MEAN)) {
        if (QC_col_MEAN[[i]] >= 0 & QC_col_MEAN_lower_bound[[i]] < 0) {
          # Adjust lower bound value to zero
          QC_col_MEAN_lower_bound[[i]] <- 0
        }
      }
      
      # CALCULATE COLUMN-WISE MEDIANS AND SD-BASED CONFIDENCE INTERVALS
      QC_col_MEDIAN <-
        QC_selected_ecosys_ds %>% select_if(is.numeric) %>%
        summarise_all( ~ median(., na.rm = TRUE))
      QC_col_MEDIAN_upper_bound <-
        QC_col_MEDIAN + (QC_acc_var_SD * QC_col_SD)
      QC_col_MEDIAN_lower_bound <-
        QC_col_MEDIAN - (QC_acc_var_SD * QC_col_SD)
      # Adjusting lower bounds with respect to column-wise medians (see 
      # explanation above)
      for (t in 1:length(QC_col_MEDIAN)) {
        if (QC_col_MEDIAN[[t]] >= 0 & QC_col_MEDIAN_lower_bound[[t]] < 0) {
          QC_col_MEDIAN_lower_bound[[t]] <- 0
        }
      }
      
      # Subset the data frame to be checked for meaningful columns (i.e. remove 
      # plot designations and the like) while preventing the conversion to a 
      # vector in case only one column remains
      QC_selected_ecosys_ds <-
        subset(
          QC_selected_ecosys_ds,
          select = names(QC_col_MEAN_upper_bound),
          drop = FALSE
        )
      
      # FLAG VALUES THAT ARE OUTSIDE THE SPECIFIED CONFIDENCE INTERVALS
      # Define QC logic and check functions
      QC_logic_function <-
        function(element, lower_bound, upper_bound) {
          ifelse(is.na(element),
                 FALSE,
                 element < lower_bound | element > upper_bound)
        }
      
      QC_check_function <- function(lst) {
        QC_logic_function(lst[[1]], lst[[2]], lst[[3]])
      }
      
      # MEAN-BASED OUTLIER DETECTION
      QC_df_list_MEAN <-
        lapply(names(QC_selected_ecosys_ds), function(index) {
          list(QC_selected_ecosys_ds[[index]],
               QC_col_MEAN_lower_bound[[index]],
               QC_col_MEAN_upper_bound[[index]])
        })
      
      QC_outside_MEAN_CI <-
        as.data.frame(Map(QC_check_function, QC_df_list_MEAN))
      colnames(QC_outside_MEAN_CI) <- names(QC_selected_ecosys_ds)
      
      # MEDIAN-BASED OUTLIER DETECTION
      QC_df_list_MEDIAN <-
        lapply(names(QC_selected_ecosys_ds), function(index) {
          list(QC_selected_ecosys_ds[[index]],
               QC_col_MEDIAN_lower_bound[[index]],
               QC_col_MEDIAN_upper_bound[[index]])
        })
      
      QC_outside_MEDIAN_CI <-
        as.data.frame(Map(QC_check_function, QC_df_list_MEDIAN))
      colnames(QC_outside_MEDIAN_CI) <- names(QC_selected_ecosys_ds)
      
      # Export data sets with flagged observations to the global environment for
      # inspection and append to list
      if (any(QC_outside_MEAN_CI == TRUE)) {
        QC_outside_MEAN_CI <- QC_outside_MEAN_CI %>%
          select_if(function(col)
            any(col == TRUE))
        
        assign(paste0("QC_", as.character(u), "_flagged_MEAN_", p),
               QC_outside_MEAN_CI,
               envir = .GlobalEnv)
        QC_outside_CI_mean_list[[paste0(as.character(u), "_", p)]] <-
          QC_outside_MEAN_CI
      }
      
      if (any(QC_outside_MEDIAN_CI == TRUE)) {
        QC_outside_MEDIAN_CI <- QC_outside_MEDIAN_CI %>%
          select_if(function(col)
            any(col == TRUE))
        
        assign(paste0("QC_", as.character(u), "_flagged_MEDIAN_", p),
               QC_outside_MEDIAN_CI,
               envir = .GlobalEnv)
        QC_outside_CI_median_list[[paste0(as.character(u), "_", p)]] <-
          QC_outside_MEDIAN_CI
      }
    }
  }
  
  # Prompt about QC results
  if (length(QC_outside_CI_mean_list) > 0 |
      length(QC_outside_CI_median_list) > 0) {
    cat(
      "Warning: Some data sets were flagged because variables had larger variance than allowed. Please check all flagged data sets before moving on.\n"
    )
  } else {
    cat(
      "Info: No data sets were flagged for having variance larger than allowed. If no other warnings were prompted, you can continue safely.\n"
    )
  }
} else {
  cat("Info: No data sets flagged for quality control. Skipping ...\n")
}

# ------------------------------------------------------------------------------
# PERFORM DATA SET-INTERN AGGREGATIONS (DIA)
# ------------------------------------------------------------------------------
# Dataset-intern aggregation is performed before group-intern aggregation (GIA).
# Depending on data structure, the following aggregation approaches are supported:
# - DIA = 2: Plot-wise aggregation of data organized in long format. Currently
#   supported are up to three grouping variables (plot + two additional grouping
#   variables)
# - DIA = 3: Plot-wise aggregation of data in wide format; any number of columns
#   is supported

DIA_ds_list <- subset(datasets_table, datasets_table$DIA == "yes")

if (nrow(DIA_ds_list) > 0) {
setwd(here("Processing"))
cat("Info: Performing data set-intern aggregations ...\n")
for (p in DIA_ds_list$BaseID) {
  # Prompt about the pipeline's progress
  cat("Info: Processing data set", p, "...\n")
  # PERFORM DATA SET PREPARATION
  # Get aggregation variables
  DIA_selected_entry <- subset(DIA_ds_list, DIA_ds_list$BaseID == p)
  DIA_grouping_vars <-
    c(
      DIA_selected_entry$DIAcol1,
      DIA_selected_entry$DIAcol2,
      DIA_selected_entry$DIAcol3
    )
  DIA_grouping_vars <-
    DIA_grouping_vars[DIA_grouping_vars != "" &
                        !is.na(DIA_grouping_vars)]
  # Open data set
  DIA_selected_ds <-
    read.csv(dir(pattern = paste0("^", p, "_")), check.names = FALSE)
  DIA_start_focal_variables <-
    which(colnames(DIA_ds_list) == "C1")
  DIA_focal_variables <-
    DIA_selected_entry[, DIA_start_focal_variables:ncol(DIA_selected_entry)]
  DIA_focal_variables <-
    DIA_focal_variables %>% 
    select_if(~ !all(is.na(.x) | .x == "")) %>% 
    as.character() %>% 
    as.vector()
  # Subset for focal variables
  DIA_selected_ds <-
    DIA_selected_ds[, DIA_focal_variables]

  # PERFORM AGGREGATIONS
  # DIA = 2
  if (DIA_selected_entry$DIAappr == 2) {
    DIA2_selected_ds <- DIA_selected_ds
    
    # Perform aggregations and correct MAD calculations
    DIA2_aggr <- DIA2_selected_ds %>%
      group_by_at(DIA_grouping_vars) %>%
      summarise_all(c("mean", "median", "sd", "mad"), na.rm = TRUE) %>% 
      mutate_at(vars(ends_with("_mad")), funs(./1.4826))
  
    DIA_produced_variables <- colnames(DIA2_aggr)
    DIA_selected_aggr <- DIA2_aggr
  }
  
  # DIA = 3
  if (DIA_selected_entry$DIAappr == 3) {
    DIA3_selected_ds <- DIA_selected_ds
    
    # Perform aggregations and correct MAD calculations
    DIA3_aggr <- DIA3_selected_ds %>%
      rowwise() %>%
      mutate(mean = mean(c_across(DIA_focal_variables[2:length(DIA_focal_variables)]))) %>%
      mutate(median = median(c_across(DIA_focal_variables[2:length(DIA_focal_variables)]))) %>%
      mutate(sd = sd(c_across(DIA_focal_variables[2:length(DIA_focal_variables)]))) %>%
      mutate(mad = mad(c_across(DIA_focal_variables[2:length(DIA_focal_variables)]))) %>%
      ungroup() %>%
      select(DIA_focal_variables[1], mean, median, sd, mad) %>% 
      mutate(mad = mad / 1.4826)
    
    DIA_produced_variables <- colnames(DIA3_aggr)
    DIA_selected_aggr <- DIA3_aggr
  }
  
  # EXPORTING AGGREGATED DATA SETS
  # Get original file name
  DIA_original_ds <-
    list.files(getwd(),
               pattern = paste0("^", p),
               full.names = TRUE)
  # Overwrite original data set with the newly-generated one
  write.csv(DIA_selected_aggr, DIA_original_ds, row.names = FALSE)
  
  # UPDATE OVERVIEW TABLE
  DIA_selected_entry[, c("DIAappr", "DIAcol1", "DIAcol2", "DIAcol3")] <- ""
  DIA_selected_entry[, c("DIA")] <- "no"
  DIA_selected_entry[, DIA_start_focal_variables:ncol(DIA_selected_entry)] <- ""
  # Add the names of the newly-generated variables
  DIA_selected_entry[, DIA_start_focal_variables:(DIA_start_focal_variables + length(DIA_produced_variables) - 1)] <-
    DIA_produced_variables
  # Overwrite old datasets_table entry with the newly-generated one
  datasets_table <- datasets_table[!datasets_table$BaseID == p, ]
  datasets_table <- rbind(datasets_table, DIA_selected_entry)
}
} else {
  cat("Info: No data sets flagged for dataset-intern aggregation. Skipping ...\n")
}

# ------------------------------------------------------------------------------
# PERFORM GROUP-INTERN AGGREGATIONS (GIA)
# ------------------------------------------------------------------------------
# Group-intern aggregation summarizes multi-year measurements distributed across
# multiple data sets. Noteworthy, abundance data sets might have been shortened
# by removing non-existing plot x taxonomic units combinations. These can be re-
# stored by setting 'GIAabcorr' in paramMAIN to 'yes'. Additionally, taxonomic 
# units not present across all data sets of a group can be introduced by setting
# 'GIAtaxcorr' to 'yes'.

GIA_ds_list <- subset(datasets_table, datasets_table$GIA == "yes")

if (nrow(GIA_ds_list) > 0) {
  setwd(here("Processing"))
  cat("Info: Performing group-intern aggregations ...\n")
  for (r in unique(GIA_ds_list$Group)) {
    # LOAD AND SUBSET DATA FRAME
    # Prompt about the pipeline's progress
    cat("Info: Processing group", r, "...\n")
    # Load group from datasets_table
    GIA_group_list <- list()
    GIA_group_sub <- subset(GIA_ds_list, GIA_ds_list$Group == r)
    GIA_groupIDs <- c(GIA_group_sub[, "BaseID"])
    # Grouping vars are the same for all data sets of a group. Thus, taking the
    # first one is representative for the whole group
    GIA_groupings_vars <- GIA_group_sub[1, ]
    GIA_groupings_vars <-
      c(GIA_groupings_vars$GIAcol1,
        GIA_groupings_vars$GIAcol2,
        GIA_groupings_vars$GIAcol3)
    GIA_groupings_vars <-
      GIA_groupings_vars[GIA_groupings_vars != "" & !is.na(GIA_groupings_vars)]
    
    # For taxonomic correction (GIAtaxcorr), get a list of taxonomic units present
    # across all data sets of the group
    if (unique(GIA_group_sub$GIAtaxcorr) == "yes") {
      GIA_taxcorr_variants <- c()
      for (s in GIA_groupIDs) {
        GIA_selected_ds <-
          read.csv(dir(pattern = paste0("^", s, "_")), check.names = FALSE)
        # Subset the selected data set for the desired variables
        GIA_variables_start <-
          which(colnames(GIA_group_sub) == "C1")
        GIA_focal_variables <-
          GIA_group_sub[1, GIA_variables_start:ncol(GIA_group_sub)]
        GIA_focal_variables <-
          c(unique(GIA_focal_variables[GIA_focal_variables != "" &
                                         !is.na(GIA_focal_variables)]))
        GIA_selected_ds <-
          GIA_selected_ds[, (colnames(GIA_selected_ds) %in% GIA_focal_variables)]
        
        # Extract taxonomic units and append to array
        GIA_variants_ds <-
          unique(GIA_selected_ds[, GIA_groupings_vars[2]])
        GIA_taxcorr_variants <- c(GIA_taxcorr_variants, GIA_variants_ds)
      }
    GIA_taxcorr_variants <- unique(GIA_taxcorr_variants)
    }
    
    # Process group by data set
    for (w in GIA_groupIDs) {
      GIA_selected_ds <-
        read.csv(dir(pattern = paste0("^", w, "_")), check.names = FALSE)
      # Subset the selected data set for the desired variables
      GIA_variables_start <- which(colnames(GIA_group_sub) == "C1")
      GIA_focal_variables <-
        GIA_group_sub[1, GIA_variables_start:ncol(GIA_group_sub)]
      GIA_focal_variables <-
        c(unique(GIA_focal_variables[GIA_focal_variables != "" &
                                       !is.na(GIA_focal_variables)]))
      GIA_selected_ds <-
        GIA_selected_ds[, (colnames(GIA_selected_ds) %in% GIA_focal_variables)]
      
       # Get the name of the values column for downstream reconstruction of
       # column names
       GIA_val_col_name <-
          GIA_focal_variables[!(GIA_focal_variables %in% GIA_groupings_vars)]
      
      # Perform abundance correction by restoring the plot x taxonomic units
      # matrix fully. Missing combinations are filled with zeros; plots not
      # present are not artificially introduced; NAs are preserved. Subsequently,
      # the data is reshaped back to long format.
      if (unique(GIA_group_sub$GIAabcorr) == "yes") {
        GIA_selected_ds <-
          dcast(as.data.table(GIA_selected_ds),
            as.formula(paste(GIA_groupings_vars[1], "~", GIA_groupings_vars[2])),
            value.var = GIA_val_col_name,
            fill = 0)
      
        GIA_selected_ds <-
          melt(GIA_selected_ds,
            id.vars = GIA_groupings_vars[1],
            variable.name = GIA_groupings_vars[2],
            value.name = GIA_val_col_name,
            na.rm = FALSE)
        }
        
      # If indicated, identify and recover missing taxonomic units for the 
      # correct calculation of multi-year summary statistics
      if (unique(GIA_group_sub$GIAtaxcorr) == "yes") {
        GIA_selected_ds <-
          dcast(as.data.table(GIA_selected_ds),
                as.formula(paste(GIA_groupings_vars[1], "~", GIA_groupings_vars[2])),
                value.var = GIA_val_col_name,
                fill = 0)
        
        GIA_dropped_variants <-
          GIA_taxcorr_variants[!(GIA_taxcorr_variants %in% names(GIA_selected_ds))]
        
        GIA_fill_variants_df <-
          as.data.frame(matrix(
            data = 0L,
            nrow = nrow(GIA_selected_ds),
            ncol = length(GIA_dropped_variants)))
        colnames(GIA_fill_variants_df) <- GIA_dropped_variants
        
        GIA_selected_ds <-
          cbind(as.data.frame(GIA_selected_ds), GIA_fill_variants_df)

        # Reshape back to long format; restore original header names
        GIA_selected_ds <-
          melt(as.data.table(GIA_selected_ds),
            id.vars = GIA_groupings_vars[1],
            variable.name = GIA_groupings_vars[2],
            value.name = GIA_val_col_name,
            na.rm = FALSE)
        GIA_selected_ds <- as.data.frame(GIA_selected_ds)
        }
      
      # Add data set IDs to an extra column, allowing for a tracing back of
      # information, and store data sets in a list for rbinding
      GIA_multiyear_ID <-
        str_extract(grep(list.files(),
                         pattern = w,
                         value = TRUE), pattern = "^[[:digit:]]+_[[:digit:]]+")
      GIA_selected_ds$ID <- GIA_multiyear_ID
      GIA_group_list <- c(GIA_group_list, list(GIA_selected_ds))
    }
    
    # Expunge group-specific variants information to be filled for the next
    # GIA group
    GIA_taxcorr_variants <- c()
    
    # rbind all data sets belonging to the same group and free up memory by
    # deleting the list of data sets no longer needed
    GIA_datasets_combined <- do.call("rbind", GIA_group_list)
    rm(list = c("GIA_group_list"))
    invisible(gc(full = TRUE))
    
    # AGGREGATE DATA SETS
    # Calculate group-wise mean, median, SD and MAD
    # Non-numeric values and the last column containing data set IDs are spared
    GIA_datasets_combined$ID <- NULL
    GIA_aggr <- GIA_datasets_combined %>%
      group_by_at(GIA_groupings_vars) %>%
      summarise_all(c("mean", "median", "sd", "mad"), na.rm = TRUE) %>%
      mutate_at(vars(ends_with("_mad")), funs(. / 1.4826))
    
    # UPDATE OVERVIEW TABLE
    # Get the first data set entry for updating datasets_table; delete all other
    # entries and the corresponding outdated input files
    GIA_variables_produced <- colnames(GIA_aggr)
    GIA_retained_ID <- GIA_groupIDs[1]
    GIA_selected_entry <-
      subset(datasets_table,
             datasets_table$BaseID == GIA_retained_ID)
    datasets_table <-
      datasets_table[!(datasets_table$BaseID %in% GIA_groupIDs), ]
    
    GIA_selected_entry[, c("GIA")] <- "no"
    GIA_selected_entry[, c("GIAcol1", "GIAcol2", "GIAcol3",
                           "GIAabcorr", "GIAtaxcorr")] <- ""
    GIA_selected_entry[, GIA_variables_start:ncol(GIA_selected_entry)] <-
      ""
    
    # Add the names of the newly-generated variables
    GIA_selected_entry[, GIA_variables_start:(GIA_variables_start + 
                    length(GIA_variables_produced) - 1)] <- GIA_variables_produced
    # Add updated entry to datasets_table
    datasets_table <- rbind(datasets_table, GIA_selected_entry)
    
    # OVERWRITE OBSOLETE DATA SETS
    GIA_deleted_IDs <- GIA_groupIDs[-1]
    for (l in GIA_deleted_IDs) {
      GIA_original_ds <-
        list.files(getwd(), pattern = paste0("^", l, "_"), full.names = TRUE)
      
      if (length(GIA_original_ds) == 1) {
        file.remove(GIA_original_ds)
        cat("Info: Deleted data set", l, "from group", r, "post-merging.\n")
      }
      
      if (length(GIA_original_ds) > 1) {
        cat("Warning: Multiple data sets with base ID", l,
          "were found. Nothing was deleted. Check your data.\n")
      }
      
      if (length(GIA_original_ds) == 0 | is.null(GIA_original_ds)) {
        cat("Warning: No data set with base ID", l, "was found. Check your data.\n")
      }
    }
    
    # EXPORT AGGREGATED DATA SETS
    # Get original file name
    GIA_original_ds <-
      list.files(getwd(), pattern = paste0("^", GIA_retained_ID, "_"), 
                 full.names = TRUE)
    # Overwrite original data set with the newly-generated one
    write.csv(GIA_aggr, GIA_original_ds, row.names = FALSE)
  }
} else {
  cat("Info: No data sets flagged for group-intern aggregation. Skipping ...\n")
}

# Remove large obsolete objects from the global environment and free up memory.
# Warning messages about non-existing objects are silenced.
suppressWarnings({
  rm(list = c(
      "GIA_aggr",
      "GIA_datasets_combined",
      "GIA_selected_ds",
      "GIA_fill_variants_df"))
})
invisible(gc(full = TRUE))

# ------------------------------------------------------------------------------
# CLIM: LOAD AND SUMMARIZE CLIMATE DATA
# ------------------------------------------------------------------------------
# Climate data must be obtained from BExIS Climate Tool as yearly aggregates 
# ("Aggregation of time = year" in the climate tools' settings). For all variables
# featured in this data set, multi-year mean, median, SD, MAD, min, and max values
# are calculated.

# Important: 
# - As BExIS Climate Tool is constantly being developed, the number of entries as
#   well as their order in the exported file might vary. It is impossible to fully
#   account for these changes in data and metadata columns coding-wise. Hence, it
#   is in the user's responsibility to modify the code below if indicated.
# - Keep data columns (incl. their order) unchanged! Otherwise, qualitycounter 
#   information will no longer match the variables provided. However, row-wise 
#   operations are permitted.

setwd(here("Processing"))
CLIM_ID <- 19007

if (CLIM_ID %in% datasets_table$BaseID) {
  CLIM <-
    read.csv(dir(pattern = paste0("^", CLIM_ID, "_")), check.names = FALSE)
  
  # Subset data for focal variables; plots not in agreement with the BEpipeR mode
  # specified are not excluded, as the gain in processing speed is quite small
  CLIM_selected_entry <-
    subset(datasets_table, datasets_table$BaseID == CLIM_ID)
  CLIM_variables_start <- which(colnames(CLIM_selected_entry) == "C1")
  CLIM_focal_variables <-
    CLIM_selected_entry[, CLIM_variables_start:ncol(CLIM_selected_entry)]
  CLIM_focal_variables <-
    c(unique(CLIM_focal_variables[CLIM_focal_variables != "" &
                                    !is.na(CLIM_focal_variables)]))
  CLIM <- CLIM[, (colnames(CLIM) %in% CLIM_focal_variables)]
  
  # IF AVAILABLE, INCORPORATE INTERPOLATION INFORMATION IN REMOVING NON-TRUSTWORTHY
  # DATA POINTS
  # Isolate metadata and remove from climate data for easier processing
  CLIM_metadata_columns <- c("plotID", "year")
  CLIM_metadata <- CLIM[, CLIM_metadata_columns]
  CLIM <- CLIM[, !names(CLIM) %in% CLIM_metadata_columns]
  
  cat("Info: Processing climate data ...\n")
  if ("qualitycounter" %in% colnames(CLIM)) {
    cat("Info: 'qualitycounter' column found. Removing poorly supported data points ...\n")
    CLIM_quality_strings <- str_split(CLIM$qualitycounter, "_")
    CLIM_qualitycounter_lengths <-
      t(as.data.frame(lapply(CLIM_quality_strings, length)))
    CLIM_min_qualitycounter_length <-
      min(CLIM_qualitycounter_lengths[, 1])
    
    # Fetch and prepare processing stats
    CLIM_replacement_counter <- 0
    CLIM_nonNA_count <- CLIM %>%
      select(where(~ !is.character(.)))
    CLIM_nonNA_count <- sum(!is.na(CLIM_nonNA_count))
    
    # -1 because the 'qualitycounter' column remains
    if (CLIM_min_qualitycounter_length == (ncol(CLIM) - 1)) {
      for (row in 1:nrow(CLIM)) {
        # Spare the 'qualitycounter' column from being checked
        for (column in 1:(ncol(CLIM) - 1)) {
          CLIM_quality_info <- CLIM_quality_strings[[row]][column]
          CLIM_aggr_points <-
            as.numeric(str_replace(
              str_extract(CLIM_quality_info, pattern = "^c[[:digit:]]+"),
              "c",
              ""
            ))
          CLIM_interpol_points <-
            as.numeric(str_replace(
              str_extract(CLIM_quality_info, pattern = "i[[:digit:]]+$"),
              "i",
              ""
            ))
          
          if (!is.na(CLIM_aggr_points) &
              !is.na(CLIM_interpol_points)) {
            # Warn if there a more interpolation than aggregation points
            if (CLIM_aggr_points < CLIM_interpol_points) {
              cat(
                paste0(
                  "Warning: There a more interpolation than total data points for cell ",
                  row,
                  " ",
                  column,
                  ". This is impossible. Check your data.\n"
                )
              )
            }
            # Check if more than 60% of data points are interpolated; if so,
            # replace cell value with NA
            if (((100 / CLIM_aggr_points) * CLIM_interpol_points) > 60) {
              CLIM[row, column] <- NA
              CLIM_replacement_counter <- CLIM_replacement_counter + 1
            }
          }
        }
      }
      # Inform the user about the number of replaced climate data points
      cat(
        paste0(
          "Info: Replaced ",
          CLIM_replacement_counter,
          "/",
          CLIM_nonNA_count,
          " climate data points with NAs.\n"
        )
      )
    } else {
      cat(
        "Warning: The number of entries in 'qualitycounter' does not match the number of non-metadata columns. Invalid subsetting performed. Check your data.\n"
      )
    }
  } else {
    cat(
      "Info: No 'qualitycounter' column found. Removal of poorly supported data points skipped.\n"
    )
  }
  
  # Join climate metadata and data. This can be done by simple cbinding, as the order
  # of rows has not been changed
  CLIM <- cbind(CLIM_metadata, CLIM)
  
  # Remove no longer needed metadata and undesired raw value columns
  CLIM <- CLIM %>%
    select(-any_of(c("year", "qualitycounter", "Ta_200_max_raw", 
                     "Ta_200_min_raw", "rH_200_max_raw", "rH_200_min_raw")))
  
  # Get the number of data points (i.e. years) underlying each plot and variable
  CLIM_aggr <- CLIM %>%
    group_by(plotID) %>%
    summarise(across(everything(), ~ sum(!is.na(.))))
  
  # ACTION POTENTIALLY REQUIRED:
  # Remove variables that do not cover at least the number of years specified. This
  # also removes columns that do not have data at all. Keep in mind: HEW51 started
  # in 2016, and hence, the number of years covered might be quite low. The minimal
  # number of years to be covered strikes a balance between obtaining accurate and
  # meaningful metrics and losing valuable variables.
  CLIM_min_years <- 4
  
  cat(
    "Info: Removing climate variables with data for less than",
    CLIM_min_years,
    "years ...\n"
  )
  CLIM_aggr <- CLIM_aggr[, colSums(CLIM_aggr < CLIM_min_years) == 0]
  
  if (ncol(CLIM_aggr) == 1) {
    cat(
      "Warning: The number-of-years-covered filter is set too stringent. Keep in mind that HEW51 was established in 2016. Have a look at the temporal coverage of your climate data and repeat this step with a proper value.\n"
    )
  } else if (ncol(CLIM_aggr) < round(length(CLIM_focal_variables) / 4 , 0)) {
    cat(
      "Warning: Only few variables passed the number-of-years-covered filter. Keep in mind that HEW51 was established in 2016."
    )
  }
  
  # Subset CLIM data for variables that have enough data points
  CLIM_selected_data <- CLIM[, names(CLIM_aggr)]
  
  # Calculate multi-year statistics while correcting MAD calculations
  CLIM_aggr_stats <- CLIM_selected_data %>%
    group_by_at("plotID") %>%
    summarise_all(c("mean", "median", "sd", "mad", "min", "max"), na.rm = TRUE) %>%
    mutate_at(vars(ends_with("_mad")), funs(. / 1.4826))
  
  # EXPORT NEWLY-GENERATED CLIMATE DATA AND UPDATE DATASETS_TABLE
  CLIM_original_ds <-
    list.files(getwd(),
               pattern = paste0("^", CLIM_ID),
               full.names = TRUE)
  write.csv(CLIM_aggr_stats, CLIM_original_ds, row.names = FALSE)
  
  CLIM_variables_produced <- colnames(CLIM_aggr_stats)
  CLIM_selected_entry[, CLIM_variables_start:ncol(CLIM_selected_entry)] <-
    ""
  CLIM_selected_entry[, CLIM_variables_start:(CLIM_variables_start + length(CLIM_variables_produced) - 1)] <-
    CLIM_variables_produced
  CLIM_selected_entry[, c("AsIt")] <- "yes"
  datasets_table <-
    subset(datasets_table, datasets_table$BaseID != CLIM_ID)
  datasets_table <- rbind(datasets_table, CLIM_selected_entry)
} else {
  cat("Info: No climate data set found in datasets_table. Skipping ...\n")
}

# ------------------------------------------------------------------------------
# RAREFY ABUNDANCE DATA (RF) AND CALCULATE SPECIES DIVERSITY INDICES (DI)
# ------------------------------------------------------------------------------
# Amplicon data poses two problems that complicate data handling: i) sparsity
# (i.e. the presence of many zeros), and ii) compositionality (the non-independence
# of information within samples). With [1], a controversy begun about proper
# normalization approaches, challenging rarefaction as the quasi-standard. Since 
# then, a vast number of approaches has been proposed (for reviews see [2-4]). 
# They include various transformations (e.g. variance stabilizing transformation 
# [vst, e.g. 5], (robust) center log ratio transformation [(r)clr, 6,7])) and 
# many more sophisticated approaches that incorporate bias correction in addition
# to these transformations (e.g. ANCOM-BC [8]). However, there is still remarkable
# discord about accepted approaches with influential authors questioning the use 
# of normalization as a whole [9], to findings that position rarefaction as a
# powerful normalization approach [10].

# After sighting the literature on this in 01/2024, it was decided to use rare-
# faction as normalization approach here for the following reasons:
# - It is an easy-to-grasp concept 
# - It seems to be a proper normalization approach [10] (even though this is a 
#   matter of vivid discussion)
# - Values obtained by rarefaction can seamlessly be used in downstream steps 
#   (incl. in the calculation of various diversity indices [11]), whereas trans-
#   formation usually complicates the interpretation of results downstream or
#   even renders the calculation of diversity indices non-valid
# - It is by far the most widely-used normalization technique and implemented in
#   many (microbial) pipelines as the default (e.g. in QIIME [12])
# - If OTU and (A)SV data is compared, rarefaction does decrease the discrepancy 
#   between these data types considerably, regardless of whether alpha or beta 
#   diversity patterns are investigated [11,13]
# - The introduction of variation by random subsampling can be alleviated by repeated
#   rarefaction [14], which is performed here.

# Literature
# [1]  https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003531
# [2]  https://www.nature.com/articles/s41522-020-00160-w
# [3]  https://link.springer.com/article/10.1186/s40168-017-0237-y
# [4]  https://www.tandfonline.com/doi/full/10.1080/19490976.2023.2244139
# [5]  https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13115
# [6]  https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1982.tb01195.x
# [7]  https://journals.asm.org/doi/full/10.1128/msystems.00016-19
# [8]  https://www.nature.com/articles/s41467-020-17041-7
# [9]  https://www.frontiersin.org/articles/10.3389/fmicb.2017.02224/full
# [10] https://journals.asm.org/doi/10.1128/msphere.00354-23
# [11] https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0233872
# [12] https://www.nature.com/articles/nmeth.f.303.]
# [13] https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0264443
# [14] https://www.nature.com/articles/s41598-021-01636-1

# As the data sets flagged for rarefaction are a subset of the ones flagged for
# calculating diversity indices, rarefaction can be performed in the DI loop
DI_ds_list <- subset(datasets_table, datasets_table$DI == "yes")

if (nrow(DI_ds_list) > 0) {
  setwd(here("Processing"))
  cat("Info: Performing computation of species diversity indices ...\n")
  for (f in DI_ds_list$BaseID) {
    # Prompt about the pipeline's progress
    cat("\nInfo: Processing data set", f, "...\n")
    # LOAD AND SUBSET SELECTED DATA SET
    DI_selected_ds <-
      read.csv(dir(pattern = paste0("^", f , "_")), check.names = FALSE)
    DI_selected_entry <-
      subset(DI_ds_list, DI_ds_list$BaseID == f)
    DI_variables_start <-
      which(colnames(DI_selected_entry) == "C1")
    DI_focal_variables <-
      DI_selected_entry[, DI_variables_start:length(DI_selected_entry)]
    DI_focal_variables <-
      c(DI_focal_variables[DI_focal_variables != "" &
                             !is.na(DI_focal_variables)])
    
    # Subset the selected data set for the focal mean (!) variable produced by up-
    # stream data aggregation. If no mean variable is found, this step is skipped.
    # As DIAcol information is no longer available, use the first two focal variables
    # for the grouping.
    DI_selected_ds <- DI_selected_ds[, DI_focal_variables]
    DI_grouping_vars <- DI_focal_variables[1:2]
    
    # Warn if the grouping variables selected are non-characters
    if (!is.character(typeof(DI_selected_ds[, 1])) |
        !is.character(typeof(DI_selected_ds[, 2]))) {
      cat(
        paste0(
          "Warning: ",
          f,
          " - The grouping variables selected are non-characters. Check your data.\n"
        )
      )
    }
    
    DI_mean_columns <-
      grep("mean$", names(DI_selected_ds), value = TRUE)
    
    if (length(DI_mean_columns) > 0) {
      DI_selected_ds_mean <-
        DI_selected_ds[, c(DI_grouping_vars, DI_mean_columns)]
    } else {
      DI_selected_ds_mean <- DI_selected_ds
    }
    
    # RESHAPE DATA SETS
    # ACTION POTENTIALLY REQUIRED:
    # Combinations not present are filled with zeros (!). By default, all DI data
    # sets are reshaped. To prevent this, they can be whitelisted. To do so, fill
    # the following vector with data set base IDs to be skipped in reshaping:
    DI_reshape_whitelist <- c()
    
    if (!(f %in% DI_reshape_whitelist)) {
      # Due to consistent handling, the values column is always the third one.
      # Hence, warn if the data set has unequal to three columns.
      DI_ncols_allowed <- 3
      if (ncol(DI_selected_ds_mean) != DI_ncols_allowed) {
        cat(
          "Warning:",
          f,
          "has more than",
          DI_ncols_allowed,
          "columns. This hints towards issues in upstream aggregation and subsetting. Check your data.\n"
        )
      }
      
      cat("Info: Reshaping data set ...\n")
      DI_ds_RES <-
        dcast(
          as.data.table(DI_selected_ds_mean),
          as.formula(paste(
            names(DI_selected_ds_mean)[1],
            "~" ,
            names(DI_selected_ds_mean)[2]
          )),
          value.var = names(DI_selected_ds_mean)[3],
          fill = 0
        )
      
      # Convert DI_ds_RES back to a data frame
      DI_ds_RES <- as.data.frame(DI_ds_RES)
      
    } else if (f %in% DI_reshape_whitelist) {
      DI_ds_RES <- DI_selected_ds_mean
    }
    
    # PERFORM REPEATED RAREFACTION IF INDICATED
    # Rarefaction is performed in chunks to prevent memory from becoming the limiting
    # factor. On capable systems, rarefaction performance might be increased by in-
    # creasing chunk size. Prior to rarefaction, values are rounded up (!) to the
    # next integer. This is preferred to normal rounding as, otherwise, mean read/
    # sequence values of e.g. 0.49 are set to zero. Plot IDs are removed in the
    # rounding step and added back later.
    DI_ds_RES_plots <- DI_ds_RES[, 1]
    DI_ds_RES <- ceiling(DI_ds_RES[, -1])
    
    if (DI_selected_entry$RF == "yes") {
      # Construct list for holding all summary statistics
      RF_ds_RES_sort_sums_all <- list()
      # Get minimal non-zero (!) plot-based subsample size for rarefaction
      RF_raremin <- min(rowSums(DI_ds_RES)[rowSums(DI_ds_RES) > 0])
      RF_rowmax <- max(rowSums(DI_ds_RES))
      # Warn the user if the sub-sampled proportion is quite small (i.e. < 10%).
      # Due to repeated rarefaction, this warning might be overly cautious.
      RF_min_subsample_ratio <- 0.1
      RF_subsample_ratio <- (RF_raremin / RF_rowmax)
      
      cat(
        paste0(
          "Info: Rarefaction subsampling size: ",
          RF_raremin,
          "; max sample size: ",
          RF_rowmax,
          ".\n"
        )
      )
      
      if (RF_subsample_ratio < RF_min_subsample_ratio) {
        cat(
          paste0(
            "Warning: Compared with the largest sample, the subsampling size selected results in a ratio of only ",
            round(RF_subsample_ratio * 100, 2),
            "%. This might result in inaccurate normalization by not fully capturing the variance in the larger samples.\n"
          )
        )
      }
      
      # Obtain the number of rarefactions to perform from the selected datasets_table
      # entry
      RF_nrepeats <- DI_selected_entry$RFnrep
      
      if (RF_nrepeats > 1000) {
        cat(
          paste0(
            "Warning: The number of rarefactions requested is quite high (= ",
            RF_nrepeats,
            "). This could result in long processing times.\n"
          )
        )
      } else if (RF_nrepeats < 100) {
        cat(
          paste0(
            "Warning: The number of rarefactions requested is quite low (= ",
            RF_nrepeats,
            "). This could result in sub-optimal normalization.\n"
          )
        )
      }
      
      cat(
        "Info: Performing rarefaction with",
        RF_nrepeats,
        "repetitions. This can take some time ...\n"
      )
      
      # Provide row names instead of row numbers to DI_ds_RES to be able to spot
      # the silent re-ordering of rows through rtk and define a function to fix this
      row.names(DI_ds_RES) <- DI_ds_RES_plots
      RF_reorder_plots_function <-
        function(RF_matrix, RF_plots_vector) {
          RF_matrix[RF_plots_vector, , drop = FALSE]
        }
      
      # Convert DI_ds_RES to matrix and perform repeated rarefaction
      DI_ds_RES <- as.matrix(DI_ds_RES)
      RF_ncores <- detectCores(logical = FALSE) - 1
      RF_chunksize <- 10
      RF_nchunks <- RF_nrepeats / RF_chunksize
      RF_seeds <- c(1:RF_nchunks)
      
      for (z in 1:RF_nchunks) {
        RF_ds_RES <-
          rtk(
            input = DI_ds_RES,
            repeats = RF_chunksize,
            depth = RF_raremin,
            ReturnMatrix = RF_chunksize,
            margin = 1,
            verbose = FALSE,
            threads = RF_ncores,
            tmpdir = here("Temp"),
            seed = RF_seeds[z]
          )
        
        # Re-order rows/samples, calculate cell-wise sums across the rarefied matrices
        # stored in RF_ds_RES and append to list holding summary statistics
        RF_ds_RES_sort <-
          lapply(RF_ds_RES[["raremat"]], function(RF_matrix) {
            RF_reorder_plots_function(RF_matrix, DI_ds_RES_plots)
          })
        
        RF_ds_RES_sort_sum <-
          apply(simplify2array(RF_ds_RES_sort), c(1, 2), FUN = sum)
        RF_ds_RES_sort_sums_all <-
          c(RF_ds_RES_sort_sums_all, list(RF_ds_RES_sort_sum))
      }
      
      # Calculate cell-wise means from cell-wise sums across all chunks, round to
      # the nearest integer, add back plot IDs, revert to original plot IDs header
      # and convert to data frame with integer values
      RF_ds_RES_mean_aggr <-
        apply(simplify2array(RF_ds_RES_sort_sums_all), c(1, 2), FUN = mean)
      RF_ds_RES_mean_aggr <- round(RF_ds_RES_mean_aggr, 0)
      RF_ds_RES_mean_aggr <-
        cbind(DI_ds_RES_plots, RF_ds_RES_mean_aggr)
      colnames(RF_ds_RES_mean_aggr)[1] <-
        names(DI_selected_ds_mean)[1]
      RF_ds_RES_mean_aggr <- as.data.frame(RF_ds_RES_mean_aggr)
      RF_ds_RES_mean_aggr[, -1] <-
        sapply(RF_ds_RES_mean_aggr[, -1], as.numeric)
      
      # COMPUTE RAREFACTION CURVES AND SLOPES TO ANALYSE HOW WELL VARIATION IS
      # CAPTURED USING THE SUBSAMPLE SIZE SELECTED
      # Keep in mind that these visualizations are performed using the vegan package
      # and are based on non-repeated rarefaction. Still, they should be sufficient
      # for getting the overall picture.
      
      # Set the step size for plotting rarefaction curves adaptively to speed up
      # processing
      if (RF_raremin <= 10) {
        RF_stepsize <- 1
      } else if (RF_raremin <= 100) {
        RF_stepsize <- 10
      } else if (RF_raremin <= 1000) {
        RF_stepsize <- 50
      } else if (RF_raremin <= 10000) {
        RF_stepsize <- 200
      } else if (RF_raremin <= 100000) {
        RF_stepsize <- 4000
      } else if (RF_raremin <= 1000000) {
        RF_stepsize <- 100000
      } else if (RF_raremin > 1000000) {
        RF_stepsize <- 200000
      }
      
      # Rarefaction curves
      setwd(here("Output"))
      cat("Info: Computing rarefaction curves and slopes ...\n")
      png(
        filename = paste0(
          "RF_",
          f,
          "_rarefaction_curves_subsample_",
          as.character(RF_raremin),
          ".png"
        ),
        width = 8,
        height = 6,
        units = "in",
        res = 300
      )
      
      suppressWarnings({
        rarecurve(
          DI_ds_RES,
          step = RF_stepsize,
          RF_raremin,
          xlab = "Subsample size",
          ylab = "Diversity",
          col = "red",
          label = FALSE,
          lty = "dotted",
          lwd = 1
        )
      })
      dev.off()
      
      # Rarefaction slopes
      suppressWarnings(RF_rareslope <-
                         as.data.frame(rareslope(DI_ds_RES, RF_raremin)))
      RF_rareslope <- cbind(rownames(RF_rareslope),
                            data.frame(RF_rareslope, row.names = NULL))
      names(RF_rareslope) <- c("Plot", "Rarefaction_slope")
      
      write.csv(
        RF_rareslope,
        paste0(
          "RF_",
          f,
          "_rarefaction_slopes_subsample_",
          as.character(RF_raremin),
          ".csv"
        ),
        row.names = FALSE
      )
      
      # Make DI_ds_RES the endpoint of the rarefaction step, holding the rarefaction
      # results
      DI_ds_RES <- RF_ds_RES_mean_aggr
      
    } else if (DI_selected_entry$RF == "no") {
      # If no rarefaction was performed, continue with DI_ds_RES, add back plot IDs
      # as extra column and as row names and harmonize plot IDs header
      DI_ds_RES <- cbind(DI_ds_RES_plots, DI_ds_RES)
      row.names(DI_ds_RES) <- DI_ds_RES_plots
      names(DI_ds_RES)[1] <- DI_focal_variables[1]
    }
    
    # CALCULATE DIVERSITY INDICES
    # If rarefaction was performed, use the rarefied abundance matrix for calculating
    # diversity indices. If not, calculate these metrics on the non-rarefied data.
    # For more information on the calculated indices, see the abdiv R package manual.
    cat("Info: Calculating diversity indices. This can take some time ...\n")
    
    # Start cluster for parallel processing; this ~ doubles processing performance.
    # Spurious parallelization warnings in calculating diversity indices are suppressed
    DI_cluster <- makeCluster(detectCores(logical = FALSE) - 1)
    registerDoSNOW(DI_cluster)
    
    # Richness
    DI_richness <-
      as.data.frame(specnumber(DI_ds_RES[, -1], MARGIN = 1))
    DI_richness <- cbind(rownames(DI_richness),
                         data.frame(DI_richness, row.names = NULL))
    names(DI_richness) <- c(names(DI_ds_RES[1]), "Richness")
    
    # Filter out empty sampling units, as no meaningful diversity indices can be
    # calculated on these. The plots in this data frame are used for downstream
    # left-joining of diversity indices, effectively removing nonsensical values.
    DI_richness <- subset(DI_richness, DI_richness$Richness > 0)
    
    # Menhinick richness index
    # + Corrects for different sample sizes
    # - Is meaningful only for raw counts, not transformed counts or proportions
    DI_menhinick_function <- function(x) {
      sum(x > 0) / sqrt(sum(x))
    }
    
    suppressWarnings({
      DI_menhinick <-
        ddply(DI_ds_RES, names(DI_ds_RES[1]), function(x) {
          data.frame(Menhinick = DI_menhinick_function(x[-1]))
        },
        .parallel = TRUE, .paropts = list(.export = "DI_menhinick_function"))
    })
    
    # Margalef richness index
    # + Corrects for differences in sample sizes and does work on all-zeros plots
    # - Meaningful only for raw counts, not transformed counts nor proportions or
    #   densities
    # - Yields the same value for all-zeros plots and plots that have a richness
    #   and abundance of one
    DI_margalef_function <-
      function(x) {
        (sum(x > 0) - 1) / log(sum(x))
      }
    
    suppressWarnings({
      DI_margalef <-
        ddply(DI_ds_RES, names(DI_ds_RES[1]), function(x) {
          data.frame(Margalef = DI_margalef_function(x[-1]))
        },
        .parallel = TRUE, .paropts = list(.export = "DI_margalef_function"))
    })
    
    # Shannon-Wiener index
    # + Most-widely used index in ecology
    # + Unlike for Simpson index, rare species/OTUs/SVs contribute to the index
    # - Only yields meaningful results for raw counts, not transformed counts nor
    #   proportions
    DI_shannon <-
      as.data.frame(diversity(DI_ds_RES[-1], index = "shannon"))
    DI_shannon <- cbind(rownames(DI_shannon),
                        data.frame(DI_shannon, row.names = NULL))
    names(DI_shannon) <- c(names(DI_ds_RES[1]), "Shannon")
    
    # Simpson index
    # + Widely-used index in ecology
    # - Heavily weighted towards the most abundant species/OTU/SV (i.e. the addition
    #   of rare species/OTUs/SVs does not change the index)
    DI_simpson <-
      as.data.frame(diversity(DI_ds_RES[-1], index = "simpson"))
    DI_simpson <- cbind(rownames(DI_simpson),
                        data.frame(DI_simpson, row.names = NULL))
    names(DI_simpson) <- c(names(DI_ds_RES[1]), "Simpson")
    
    # Inverse Simpson index
    DI_simpson_inv <-
      as.data.frame(diversity(DI_ds_RES[-1], index = "invsimpson"))
    DI_simpson_inv <- cbind(rownames(DI_simpson_inv),
                            data.frame(DI_simpson_inv, row.names = NULL))
    names(DI_simpson_inv) <- c(names(DI_ds_RES[1]), "Simpson_inv")
    
    # Stop the snow cluster to prevent issues
    stopCluster(DI_cluster)
    
    # Join species diversity data for plots with richness > 1, effectively removing
    # empty sampling units
    DI_indices_list <-
      list(DI_richness,
           DI_menhinick,
           DI_margalef,
           DI_shannon,
           DI_simpson,
           DI_simpson_inv)
    
    DI_indices_complete <-
      join_all(
        DI_indices_list,
        by = names(DI_richness[1]),
        type = "left",
        match = "all"
      )
    
    # Check if additional rows were introduced in left-joining. This would hint
    # towards issues in data aggregation
    if (nrow(DI_indices_complete) != nrow(DI_richness)) {
      cat(
        "Warning: In joining diversity indices, rows were added or deleted. Check your data.\n"
      )
    }
    
    # Check the indices computed for suspicious values. As empty sampling units
    # were removed, no NA, NaN or Inf values should be present
    if (any(sapply(DI_indices_complete, function(x)
      any(is.na(x) | is.nan(x) | is.infinite(x))))) {
      cat(
        "Warning: The computed diversity indices contain NA, NaN or Inf values. This should not have happened. Check your data.\n"
      )
    }
    
    # Important: The other GIA/DIA-produced variables such as SD, median, and MAD
    # are not added back to the indices table, as they have not been rarefied and
    # these information should be implicitly be accounted for in the species diversity
    # data.
    
    # Update focal variables
    DI_variables_produced <- colnames(DI_indices_complete)
    DI_selected_entry[, DI_variables_start:ncol(DI_selected_entry)] <-
      ""
    DI_selected_entry[, DI_variables_start:(DI_variables_start + length(DI_variables_produced) - 1)] <-
      DI_variables_produced
    # Update aggregation information
    DI_selected_entry[, c("RF", "DI")] <- "no"
    DI_selected_entry[, c("RFnrep")] <- ""
    datasets_table <-
      subset(datasets_table, datasets_table$BaseID != f)
    datasets_table <- rbind(datasets_table, DI_selected_entry)
    # Export newly-generated data
    setwd(here("Processing"))
    DI_original_ds <-
      list.files(getwd(),
                 pattern = paste0("^", f),
                 full.names = TRUE)
    write.csv(DI_indices_complete, DI_original_ds, row.names = FALSE)
  }
  
  # Remove potentially large elements held in memory
  suppressWarnings({
  rm(list = c("RF_ds_RES", "RF_ds_RES_sort", "RF_ds_RES_sort_sums_all"))
    })
  invisible(gc(full = TRUE))
} else {
  cat(
    "Info: No data sets flagged for rarefaction/calculating species diversity indices. Skipping ...\n"
  )
}

# ------------------------------------------------------------------------------
# LOAD AND LEFT-JOIN (MRG) ALL DATASETS 
# ------------------------------------------------------------------------------
# Get fully processed datasets_table entries; warn if some data sets have not been
# fully processed
MRG_columns_to_check <-
  c("rSUB", "rDW", "rSTD", "rRES", "rFB", "DIA", "GIA", "RF", "DI")

if (all(datasets_table[, MRG_columns_to_check] == "no")) {
  cat(
    "Info: All data sets were processed successfully and are ready to be merged. You can continue safely.\n"
  )
} else {
  cat(
    "Warning: Some data sets still need processing before merging. Check datasets_table.\n"
  )
}

# Make a copy of RES_suffixes_all to append reshaping information generated here
MRG_suffixes_all <- RES_suffixes_all

# Loop over all data sets and left-join
setwd(here("Processing"))
MRG_composite_list_EP <- list()
MRG_composite_list_GP <- list()
cat("Info: Left-joining data sets ...\n")

for (q in datasets_table$BaseID) {
  cat("Info: Processing data set", q, "...\n")
  MRG_selected_ds <-
    read.csv(dir(pattern = paste0("^", q , "_")), check.names = FALSE)
  MRG_selected_entry <-
    subset(datasets_table, datasets_table$BaseID == q)
  MRG_variables_start <-
    which(colnames(MRG_selected_entry) == "C1")
  MRG_focal_variables <-
    MRG_selected_entry[, MRG_variables_start:length(MRG_selected_entry)]
  MRG_focal_variables <-
    c(MRG_focal_variables[MRG_focal_variables != "" &
                            !is.na(MRG_focal_variables)])
  # Subset the selected data set for the focal variables specified
  MRG_selected_ds <- MRG_selected_ds[, MRG_focal_variables]
  
  # Process data sets that seem to have additional grouping variables, resulting
  # in more rows than plotIDs_template has (e.g. data set 20127)
  MRG_nrows_template <- nrow(plotIDs_template)
  if (nrow(MRG_selected_ds) > MRG_nrows_template &
      is.character(MRG_selected_ds[, 2])) {
    cat(
      "Warning: Data set",
      q,
      "has more than",
      MRG_nrows_template,
      "rows. Performing repeated re-shaping to wide format to accommodate additional factor levels. Check your data.\n"
    )
    
    # Perform repeated reshaping
    while (is.character(MRG_selected_ds[, 2]))
    {
      # Append reshaping variables to MRG_suffixes_all for removal in COMD
      MRG_suffixes <-
        paste0("_", as.character(unique(MRG_selected_ds[[names(MRG_selected_ds[2])]])))
      MRG_suffixes_all <- c(MRG_suffixes_all, MRG_suffixes)
      
      # Reshape
      MRG_selected_ds <-
        MRG_selected_ds %>%
        pivot_wider(
          names_from = names(MRG_selected_ds[2]),
          values_from = names(MRG_selected_ds[3:length(MRG_selected_ds)])
        )
    }
    
    # Check the reshaping success
    if (nrow(MRG_selected_ds) > MRG_nrows_template) {
      cat(
        "Warning: Even after repeated reshaping,",
        q,
        "has more rows than allowed. Check your data.\n"
      )
    }
    MRG_selected_ds <- as.data.frame(MRG_selected_ds)
  }
  
  # Harmonize plotIDs and add data set IDs
  # For EP plot designations (both grassland and forest)
  # The harmonized plot IDs column is given a 'fancy' and most likely unique name
  # to prevent issues with duplicated column names. The name of this column is
  # adjusted after deleting the old one.
  if (all(grepl("[AHS]E[WG][[:digit:]]+$", MRG_selected_ds[, 1]))) {
    MRG_selected_ds <- MRG_selected_ds %>%
      mutate(EP_Plotid0_MRG_BEpipeR = sprintf(
        "%s%02d",
        gsub("[^[:alpha:]]", "", as.character(MRG_selected_ds[, 1])),
        as.integer(gsub(
          "[^[:digit:]]", "", as.character(MRG_selected_ds[, 1])
        ))
      )) %>%
      select(EP_Plotid0_MRG_BEpipeR, everything())
    # Delete old plotID column by name
    MRG_selected_ds <-
      MRG_selected_ds[, !(names(MRG_selected_ds) %in% MRG_focal_variables[1])]
    # Rename plot IDs column after deleting the old one
    colnames(MRG_selected_ds)[colnames(MRG_selected_ds) == "EP_Plotid0_MRG_BEpipeR"] <-
      "EP_Plotid0"
  }
  
  # For GP plot designations (both grassland and forest)
  if (all(grepl("[AHS][[:digit:]]+$", MRG_selected_ds[, 1]))) {
    MRG_selected_ds <- MRG_selected_ds %>%
      mutate(GP_Plotid0_MRG_BEpipeR = sprintf(
        "%s%05d",
        gsub("[^[:alpha:]]", "", as.character(MRG_selected_ds[, 1])),
        as.integer(gsub(
          "[^[:digit:]]", "", as.character(MRG_selected_ds[, 1])
        ))
      )) %>%
      select(GP_Plotid0_MRG_BEpipeR, everything())
    # Delete old plotID column by name
    MRG_selected_ds <-
      MRG_selected_ds[, !(names(MRG_selected_ds) %in% MRG_focal_variables[1])]
    # Rename plot IDs column after deleting the old one
    colnames(MRG_selected_ds)[colnames(MRG_selected_ds) == "GP_Plotid0_MRG_BEpipeR"] <-
      "GP_Plotid0"
  }
  
  # Warn if mixed or no plot designations were detected
  if (any(grepl("[AHS][[:digit:]]+$", MRG_selected_ds[, 1])) &
      any(grepl("[AHS]E[WG][[:digit:]]+$", MRG_selected_ds[, 1]))) {
    cat(
      paste0(
        "Warning: EP and GP designations found in the same column for ",
        q,
        ". Check your data.\n"
      )
    )
  } else if (!any(grepl("[AHS][[:digit:]]+$", MRG_selected_ds[, 1])) &
             !any(grepl("[AHS]E[WG][[:digit:]]+$", MRG_selected_ds[, 1]))) {
    cat(paste0(
      "Warning: No valid plot designations found for ",
      q,
      ". Check your data.\n"
    ))
  }
  
  # Add data set IDs to column headers
  MRG_dataset_ID <-
    str_replace(basename(list.files(
      getwd(),
      pattern = paste0("^", q, "_"),
      full.names = TRUE
    )), pattern = "_data.csv", "")
  
  colnames(MRG_selected_ds)[-1] <-
    paste0(colnames(MRG_selected_ds)[-1], "_", MRG_dataset_ID)
  
  # Store data sets in lists; discriminate between EP and GP plot IDs scheme
  if (all(grepl("[AHS]E[WG][[:digit:]]+$", MRG_selected_ds[, 1]))) {
    MRG_composite_list_EP[[as.character(q)]] <- MRG_selected_ds
  } else if (all(grepl("[AHS][[:digit:]]+$", MRG_selected_ds[, 1]))) {
    MRG_composite_list_GP[[as.character(q)]] <- MRG_selected_ds
  }
}

# Left-join all data sets with template; discriminate between EP and GP plot IDs
# scheme
MRG_composite <-
  reduce(MRG_composite_list_EP,
         left_join,
         by = "EP_Plotid0",
         .init = plotIDs_template)
MRG_composite <-
  reduce(MRG_composite_list_GP,
         left_join,
         by = "GP_Plotid0",
         .init = MRG_composite)

# ------------------------------------------------------------------------------
# PERFORM FINAL QUALITY CONTROL (FQC)
# ------------------------------------------------------------------------------
cat("Info: Performing final quality control. Check all messages that follow carefully.\n")

# Duplicate data frame for FQC
FQC_composite <- MRG_composite

# Check left-joining success by plotID cell values
cat("Info: Checking left-joining success by plotID values ...\n")
if (identical(plotIDs_template$EP_Plotid0, FQC_composite$EP_Plotid0)) {
  cat("Info: Left-joining successful. No additional plots added. You can continue safely.\n")
} else {
  cat("Warning: Left-joining not successful. Additional plots added. Check your data.\n")
}

# Search for plot name patterns in values matrix while sparing metadata columns
# Finding these patterns would hint towards issues in data aggregation
cat("Info: Searching for plotIDs contamination in values matrix ...\n")
FQC_plotID_contamination <-
  apply(FQC_composite[, -which(names(FQC_composite) %in% names(plotIDs_template))], c(1, 2), function(x)
    any(grepl(pattern = "[[AHS]E[WG][[:digit:]]+|[AHS][[:digit:]]+", x)))

if (any(FQC_plotID_contamination)) {
  cat(
    "Warning: plotID patterns were found in the following column(s) of the values matrix:",
    colnames(FQC_plotID_contamination)[apply(FQC_plotID_contamination, 2, any)]
  )
} else {
  cat("Info: No plotID patterns were found in the values matrix. You can continue safely.\n")
}

# Report duplicated column headers
cat("Info: Searching for duplicated column headers ...\n")
FQC_dup_head <- sum(duplicated(names(FQC_composite)))
if (FQC_dup_head > 0) {
  FQC_dup_head_names <-
    unique(names(FQC_composite)[duplicated(names(FQC_composite))])
  cat(paste0(
    "Warning: Following columns are duplicated: ",
    FQC_dup_head_names,
    "\n"
  ))
} else if (FQC_dup_head == 0) {
  cat("Info: No duplicated column headers were found. You can continue safely.\n")
}

# Report headers without data set ID; spare the metadata columns
cat("Info: Searching for headers without data set ID ...\n")
FQC_miss_IDs <-
  names(FQC_composite)[!grepl("[[:digit:]]+_[[:digit:]]+$", names(FQC_composite))]
FQC_miss_IDs <- FQC_miss_IDs[!(FQC_miss_IDs %in% names(plotIDs_template))]

if (length(FQC_miss_IDs) > 0) {
  cat("Warning: The following columns do not have any data set ID:",
      FQC_miss_IDs,
      "\n")
} else if (length(FQC_miss_IDs) == 0) {
  cat("Info: All non-metadata headers have data set IDs. You can continue safely.\n")
}

# Replace spaces in column headers with underscores
cat("Info: Searching for spaces in header names ...\n")
FQC_colnames_with_spaces <-
  names(FQC_composite)[grepl("\\s+", names(FQC_composite))]

if (length(FQC_colnames_with_spaces) > 0) {
  cat(
    "Info: In the following column headers, spaces were replaced with underscores:",
    paste(FQC_colnames_with_spaces, collapse = ", "), "\n"
  )
  colnames(FQC_composite) <-
    gsub("\\s+", "_", colnames(FQC_composite))
} else if (length(FQC_colnames_with_spaces) == 0) {
  cat("Info: No column headers contain spaces.\n")
}

# Replace NaN and Inf values with NA
FQC_composite[sapply(FQC_composite, is.infinite)] <- NA
FQC_composite[sapply(FQC_composite, is.nan)] <- NA

# Exclude non-numeric columns
cat("Info: Searching for non-numeric columns ...\n")
FQC_headers_before_nonnum_excl <- colnames(FQC_composite)
FQC_composite <-
  FQC_composite[sapply(FQC_composite, is.numeric) |
                  names(FQC_composite) %in% names(plotIDs_template)]
FQC_headers_after_nonnum_excl <- colnames(FQC_composite)
FQC_nonnum_columns_removed <-
  FQC_headers_before_nonnum_excl[!(FQC_headers_before_nonnum_excl %in%
                                     FQC_headers_after_nonnum_excl)]

if (length(FQC_nonnum_columns_removed) > 0) {
  cat("Info: The following non-numeric columns were removed:",
    paste(FQC_nonnum_columns_removed, collapse = ", "), "\n")
} else if (length(FQC_nonnum_columns_removed) == 0) {
  cat("Info: No columns were non-numeric.\n")
}

# Export intermediate composite data set, because it could be of interest for quite
# some people
setwd(here("Output"))
write.csv(FQC_composite,
          "FQC_env_var_composite_intermediate.csv",
          row.names = FALSE)

# To get a composite data set ready for downstream processing, remove mono-value
# columns and those that feature NAs
# Remove mono-value columns while ignoring NAs
FQC_ncol_before_fixed_removal <- ncol(FQC_composite)
FQC_composite <-
  FQC_composite %>% select(where( ~ n_distinct(., na.rm = TRUE) > 1))
FQC_ncol_after_fixed_removal <- ncol(FQC_composite)
FQC_fixed_columns_removed <-
  FQC_ncol_before_fixed_removal - FQC_ncol_after_fixed_removal
cat("Info:", FQC_fixed_columns_removed, "mono-value columns were removed.\n")

# ACTION POTENTIALLY REQUIRED:
# Remove columns with NAs; do this with respect to the BEpipeR mode specified,
# and after excluding plots that render the obtaining of a continuous data set
# notoriously difficult (e.g. HEW51, as it was established in 2016).
# To remove plots, enter their EP designations here:
FQC_plots_to_remove <- c()

FQC_composite <-
  FQC_composite[FQC_composite$EP_Plotid0 %in% plotIDs_allowed_EP, ]

if (length(FQC_plots_to_remove) > 0) {
  FQC_composite <-
    FQC_composite[!(FQC_composite$EP_Plotid0 %in% FQC_plots_to_remove),]
  cat(
    "Info: In addition to plots not in concordance with the BEpipeR mode specified, you opted to exclude the following plots:",
    FQC_plots_to_remove, "\n")
} else if (length(FQC_plots_to_remove) == 0) {
  cat("Info: Apart from plots not in concordance with the BEpipeR mode specified, you did not opt for the exclusion of any other plots.\n")
}

FQC_ncol_before_NA_removal <- ncol(FQC_composite)
FQC_composite <-
  FQC_composite[, apply(FQC_composite, 2, function(x) ! any(is.na(x)))]
FQC_ncol_after_NA_removal <- ncol(FQC_composite)
FQC_NA_columns_removed <-
  FQC_ncol_before_NA_removal - FQC_ncol_after_NA_removal
cat(
  "Info:",
  FQC_NA_columns_removed,
  "columns contained at least one NA value and were removed.\n"
)

# Export quality-controlled and fully-filtered composite data set
write.csv(FQC_composite,
          "FQC_env_var_composite_complete.csv",
          row.names = FALSE)

# ------------------------------------------------------------------------------
# PERFORM VARIABLES SELECTION (VS) BY VARIANCE INFLATION FACTOR (VIF) ANALYSIS
# ------------------------------------------------------------------------------
# Duplicate FQC_composite for variables selection
setwd(here("Output"))
VS_composite <- FQC_composite

# Compute Pearson correlation matrix and associated P values for all variables 
# in the composite data set for downstream use in interpreting statistical models.
# This step is also helpful for detecting problems such as the same variables being
# present in multiple data sets
cat("Info: Calculating Pearson's r for all variables combinations ...\n")
VS_composite_matrix_full <- as.matrix(VS_composite)
VS_composite_matrix_vals <-
  VS_composite_matrix_full[, !(colnames(VS_composite_matrix_full) 
                               %in% names(plotIDs_template)), drop = FALSE]

VS_pearson_corr <- rcorr(VS_composite_matrix_vals, type = "pearson")
VS_pearson_corrMatr <- VS_pearson_corr$r
VS_pearson_nobs <- VS_pearson_corr$n
VS_pearson_pvals <- VS_pearson_corr$P

# Export all correlation information assembled
write.csv(VS_pearson_corrMatr, "VS_pearson_corrMat.csv", row.names = TRUE)
write.csv(VS_pearson_nobs, "VS_pearson_numbObs.csv", row.names = TRUE)
write.csv(VS_pearson_pvals, "VS_pearson_pVals.csv", row.names = TRUE)

# Flatten correlation information. For the calculation of corrected P values, the
# diagonal must be included.
VS_flattenCorrMatrix_function <- function(corrMat, pMat) {
  upper_tri <- upper.tri(corrMat, diag = TRUE)
  data.frame(
    Row  =  rownames(corrMat)[row(corrMat)[upper_tri]],
    Column  =  rownames(corrMat)[col(corrMat)[upper_tri]],
    Pearsons_r   = (corrMat)[upper_tri],
    Pvalue  =  pMat[upper_tri]
  )
}
VS_pearson_res_df <-
  VS_flattenCorrMatrix_function(VS_pearson_corrMatr, VS_pearson_pvals)

# Use FDR correction to correct P values for multiple comparisons, sort, and
# export for potential later use
VS_pearson_res_df$Pvalue_FDRcorr <-
  p.adjust(VS_pearson_res_df$Pvalue, method = "fdr")
VS_pearson_res_df <- VS_pearson_res_df[order(-VS_pearson_res_df$Pearsons_r), ]
write.csv(VS_pearson_res_df, "VS_corr_flat_complete.csv", row.names = FALSE)

# Check for highly-correlated and statistically significant comparisons (Pearson's 
# r ~ 1, corrected P value < 0.05)
VS_corr_tolerance <- 1e-10
VS_sign_threshold <- 0.05
cat(
  "Info: Checking computed Pearson correlation coefficients for statistically significant and unusually high values ...\n"
)

VS_perfect_corr <-
  subset(
    VS_pearson_res_df,
    VS_pearson_res_df$Pearsons_r >= (1 - VS_corr_tolerance) &
      VS_pearson_res_df$Pvalue_FDRcorr < VS_sign_threshold
  )

if (nrow(VS_perfect_corr) > 0) {
  cat(
    "Warning: Significant pairwise comparisons with Pearson r ~ 1 detected. Check the computed correlation output thoroughly for feasibility before you continue.\n"
  )
}

# PERFORM VARIABLES SELECTION BY VIF
# Before performing variables selection by VIF, VIF scores are usually all Inf,
# because of the high degree of co-linearity. This will be solved by vifstep().
# Here, variables selection is performed for an array of VIF thresholds to obtain 
# results that differ in their stringency of removal.

# ACTION POTENTIALLY REQUIRED:
# To protect variables from being excluded through vifstep, add their full names
# to the following character vector. This will result in their retention at all
# cost, even if they are highly correlated with other retained variables.
VS_protected_variables <- c()

if (length(VS_protected_variables) > 0 &
    all(VS_protected_variables %in% names(VS_composite))) {
  cat(
    "Info: You opted to protect the following variables in variables selection:",
    VS_protected_variables,
    "\n"
  )
} else if (length(VS_protected_variables) > 0 &
           !all(VS_protected_variables %in% names(VS_composite))) {
  cat(
    "Warning: Some variables selected for protection in variables selection could not be found in the composite data set. Check your data.\n"
  )
} else if (length(VS_protected_variables) == 0) {
  cat(
    "Info: You did not opt to protect any variables (not even plot locations) in variables selection.\n"
  )
}

# Get plot metadata column indexes; do not use value = TRUE!
VS_metadata_columns <-
  grep("^[EG]P_Plotid[0]{0,1}$", colnames(VS_composite), ignore.case = FALSE)

# PERFORM VARIABLES SELECTION
# As this step can get computationally intense, parallel processing is enabled.
# As parallelization is done through sockets (forking is not supported by Windows),
# all variables and packages used must be introduced to every worker of the cluster
VS_cluster <- makeCluster(detectCores(logical = FALSE) - 1)
clusterExport(VS_cluster, c("VS_composite", "VS_protected_variables", "VS_metadata_columns"))
invisible(clusterEvalQ(VS_cluster, {
  library(usdm) 
  library(stringr)
}))

# Check if plot metadata is confined to the first n columns and not spread across
# the composite. If the former is the case, perform variables selection. Otherwise,
# skip this step and inform the user.
if (all(diff(VS_metadata_columns) == 1) &
    VS_metadata_columns[1] == 1)  {
  
  # Define a function to perform variables selection by VIF
  VS_vif_calc_function <- function(thr) {
    VS_post_vif_data <-
      vifstep(VS_composite[, (length(VS_metadata_columns) + 1):ncol(VS_composite)],
              th = thr,
              method = "pearson",
              keep = VS_protected_variables)
    
    # Export results
    VS_analysed_vars <- as.data.frame(VS_post_vif_data@variables)
    VS_excluded_vars <- as.data.frame(VS_post_vif_data@excluded)
    VS_corr_matrix <- as.data.frame(VS_post_vif_data@corMatrix)
    VS_retained_vars_scores <- as.data.frame(VS_post_vif_data@results)
    
    # Exclude cross-correlated variables from composite data set
    VS_composite <- exclude(VS_composite, VS_post_vif_data)
    
    # Store results in list of named data sets
    VS_prod_data_frames <-
      setNames(mget(
        c("VS_analysed_vars",
            "VS_excluded_vars",
            "VS_corr_matrix",
            "VS_retained_vars_scores",
            "VS_composite")),
        c("VS_analysed_vars",
          "VS_excluded_vars",
          "VS_corr_matrix",
          "VS_retained_vars_scores",
          "VS_composite"
        ))
    
    # Add leading zeros to threshold values to facilitate the sorting of output
    # files
    thr <- str_pad(thr, width = 3, side = "left", pad = 0)
    
    # Export results
    for (u in names(VS_prod_data_frames)) {
      # Export correlation matrix with row names. For all other results, 
      # row names are omitted.
      if (u == "VS_corr_matrix") {
        write.csv(
          VS_prod_data_frames[[u]],
          file = paste0("VS_VIF", thr, "_", u, ".csv"),
          row.names = TRUE
        )
      } else {
        write.csv(
          VS_prod_data_frames[[u]],
          file = paste0("VS_VIF", thr, "_", u, ".csv"),
          row.names = FALSE
        )
      }
    }
  }
} else {
  cat(
    "Warning: PlotID columns are not confined to the first columns of the composite data set. Check your data and correct this issue. No variable selection by VIF was performed.\n"
  )
}

# Run the variables selection in parallel for the VIF thresholds provided here
VS_vif_steps <- 2:10
parLapply(VS_cluster, VS_vif_steps, VS_vif_calc_function)

# Close the cluster immediately after the parallelized processing
stopCluster(VS_cluster)

# ------------------------------------------------------------------------------
# COMPILE METADATA (COMD) FROM THE ORIGINAL DATA SETS' DATASTRUCTURE FILES
# ------------------------------------------------------------------------------
# As the compilation of descriptions for the many variables amassed is not easily 
# achieved manually, this information is compiled from the original data sets' 
# datastructure files using the jsonlite package. Note that due to aggregation, 
# not all variables' metadata might successfully be recovered. Still, after adding
# new data sets or aggregation approaches, it might be worth adjusting the lines
# that strip away these information to maximize the amount of information that is
# fetched automatically. Keep in mind that the aggregation information is nested
# (i.e. aggregation information was appended to existing variables names).

# Check that all required datastructure files are provided in 'Metadata'
setwd(here("Metadata"))
COMD_metadata_files <- str_extract(list.files(getwd(), 
           pattern = "^[[:digit:]]+_[[:digit:]]+_datastructure.txt$|.*_sensor_description.csv$", 
           full.names = FALSE), pattern = "^[[:digit:]]+")

if (length(COMD_metadata_files) > 0 &
    all(unique(datasets_table$BaseID) %in% COMD_metadata_files)) {
  cat("Info: Compiling metadata for the variables stored in the FQC composite data set ...\n")
  
  # Subset metadata_info for focal columns (which are essentially the ones checked
  # immediately before left-joining all data)
  COMD_metadata <- metadata_info
  COMD_metadata_colnames <- MRG_columns_to_check
  COMD_metadata_colnames <- c("BaseID", COMD_metadata_colnames)
  COMD_metadata <-
    COMD_metadata[, names(COMD_metadata) %in% COMD_metadata_colnames, drop = FALSE]
  
  COMD_composite_vars <-
    names(FQC_composite[, !(colnames(FQC_composite)
                            %in% names(plotIDs_template)), drop = FALSE])
  COMD_nonclim_vars_df <- data.frame()
  COMD_clim_vars_df <- data.frame()
  
  # Specify aggregation/reshaping information to be removed from variable names
  # obtained from FQC composite
  COMD_aggr_info_1 <-
    paste0("_(",
           str_replace_all(
             str_c(MRG_suffixes_all, collapse = "|"),
             pattern = "_",
             replacement = ""
           ),
           ")$")
  COMD_aggr_info_2 <- "_(mean|median|sd|mad|min|max)$"
  
  # Specify the datastructure columns of interest for downstream use
  COMD_datastructure_focal_cols <-
    c(
      "Variables.Id",
      "Variables.Label",
      "Variables.Description",
      "Variables.unit.Name",
      "Variables.unit.Description",
      "Variables.dataType.Name"
    )
  
  # Adaptively extract metadata for all variables headers
  for (x in COMD_composite_vars) {
    # Extract basic information
    COMD_fullID <-
      str_extract(x, "[[:digit:]]+_[[:digit:]]+$")
    COMD_baseID <-
      str_extract(COMD_fullID, "^[[:digit:]]+")
    COMD_isolated_var <-
      str_replace(x,
                  pattern = paste0("_", COMD_fullID),
                  replacement = "")
    # Subset COMD_metadata for the data set of interest
    COMD_metadata_sub <-
      subset(COMD_metadata,
             COMD_metadata$BaseID == COMD_baseID)
    
    # Gather aggregation information from variables' names
    if (COMD_metadata_sub$DIA == "yes" &
        COMD_metadata_sub$GIA == "no" |
        COMD_metadata_sub$DIA == "no" &
        COMD_metadata_sub$GIA == "yes" |
        COMD_baseID == 19007) {
      # If the data has been aggregated only once (incl. climate data), extraction
      # of the last aggregation sub-string is required. The second aggregation string
      # must be set to NA. Even though no RES was performed, reshaping might have
      # happened in MRG and hence, this information must be stripped first.
      COMD_trimmed_var <-
        str_replace(COMD_isolated_var, COMD_aggr_info_1, "")
      COMD_aggr_string_1 <-
        str_extract(COMD_trimmed_var, pattern = COMD_aggr_info_2)
      COMD_trimmed_var <-
        sub(COMD_aggr_info_2, "", COMD_trimmed_var)
      COMD_aggr_string_1 <- str_replace(COMD_aggr_string_1, "_", "")
      COMD_aggr_string_2 <- NA
      
    } else if (COMD_metadata_sub$DIA == "yes" &
               COMD_metadata_sub$GIA == "yes") {
      # If the data has been aggregated twice (DIA and GIA), the last two
      # aggregation sub-strings must be obtained after removing potential
      # reshaping suffixes
      COMD_trimmed_var <-
        sub(COMD_aggr_info_1, "", COMD_isolated_var)
      COMD_aggr_string_1 <-
        str_extract(COMD_trimmed_var, pattern = COMD_aggr_info_2)
      COMD_trimmed_var <-
        sub(COMD_aggr_string_1, "", COMD_trimmed_var)
      COMD_aggr_string_2 <-
        str_extract(COMD_trimmed_var, pattern = COMD_aggr_info_2)
      
      if (is.na(COMD_aggr_string_2)) {
        # If the second aggregation suffix cannot be obtained because it is the
        # start of the variable name, obtain it without the leading underscore
        COMD_aggr_string_2 <-
          str_extract(COMD_trimmed_var,
                      pattern = paste0("^", str_replace(COMD_aggr_info_2, "_", "")))
      }
      
      COMD_trimmed_var <-
        sub(COMD_aggr_string_2, "", COMD_trimmed_var)
      COMD_aggr_string_1 <- str_replace(COMD_aggr_string_1, "_", "")
      COMD_aggr_string_2 <- str_replace(COMD_aggr_string_2, "_", "")
      
    } else if (COMD_metadata_sub$DIA == "no" &
               COMD_metadata_sub$GIA == "no" &
               COMD_metadata_sub$rRES == "no" &
               COMD_baseID != 19007) {
      # If the data has not been aggregated by the pipeline at all, no aggregation
      # strings are extracted. This also means that all potential aggregation strings
      # featured in the variable's name have been there in the first place.
      COMD_aggr_string_1 <- NA
      COMD_aggr_string_2 <- NA
      COMD_trimmed_var <- COMD_isolated_var
      
    } else if (COMD_metadata_sub$rRES == "yes") {
      # If the data has been reshaped via RES (which is incompatible with DIA/GIA)
      # factor level information might be featured in column names. Remove these
      # information. If only factor level information is provided in column headers
      # the trimmed variable is NA, which does not matter.
      COMD_trimmed_var <-
        str_replace(COMD_isolated_var, COMD_aggr_info_1, "")
    }
    
    # Concatenate compiled aggregation information
    COMD_basic_meta <-
      cbind(
        COMD_baseID,
        COMD_fullID,
        x,
        COMD_trimmed_var,
        COMD_aggr_string_1,
        COMD_aggr_string_2
      )
    
    if (COMD_baseID != 19007) {
      # Fetch datastructure file for that data set and subset for the variable
      # selected
      if (exists(quote(COMD_last_ds_open))) {
        if (COMD_fullID == COMD_last_ds_open) {
          COMD_datastructure_file_df <- COMD_datastructure_file_df
        } else {
          COMD_datastructure_file_df <-
            data.frame(jsonlite::fromJSON(
              list.files(
                getwd(),
                pattern = paste0("^", COMD_fullID),
                full.names = TRUE
              ),
              flatten = TRUE
            ))
        }
      } else {
        COMD_datastructure_file_df <-
          data.frame(jsonlite::fromJSON(
            list.files(
              getwd(),
              pattern = paste0("^", COMD_fullID),
              full.names = TRUE
            ),
            flatten = TRUE
          ))
      }
      
      # Replace spaces in all variable names with underscores and subset for the
      # variable of interest
      COMD_datastructure_file_df$Variables.Label <-
        gsub("\\s+", "_", COMD_datastructure_file_df$Variables.Label)
      
      COMD_datastructure_file_df_sub <-
        subset(
          COMD_datastructure_file_df,
          COMD_datastructure_file_df$Variables.Label == COMD_trimmed_var
        )
      
      if (nrow(COMD_datastructure_file_df_sub) == 1) {
        # Subset for the columns of interest
        COMD_datastructure_file_df_sub <-
          COMD_datastructure_file_df_sub[, COMD_datastructure_focal_cols]
      } else if (nrow(COMD_datastructure_file_df_sub) == 0) {
        COMD_datastructure_file_df_sub <-
          data.frame(matrix(
            NA,
            nrow = 1,
            ncol = length(COMD_datastructure_focal_cols)
          ))
        names(COMD_datastructure_file_df_sub) <-
          COMD_datastructure_focal_cols
      } else if (nrow(COMD_datastructure_file_df_sub) > 1) {
        cat(
          "Warning: More than one match was found for",
          COMD_trimmed_var,
          "in its original datastructure file. This cannot be. Check your data and resolve this issue.\n"
        )
      }
      
      # Combine basic metadata with datastructure and aggregation information
      COMD_combined_meta <-
        cbind(COMD_basic_meta,
              COMD_datastructure_file_df_sub,
              COMD_metadata_sub)
      # Delete the BaseID column as it is no longer needed, and rename columns
      COMD_combined_meta$BaseID <- NULL
      COMD_combined_meta <- COMD_combined_meta %>%
        rename(
          "FullID" = "COMD_fullID",
          "BaseID" = "COMD_baseID",
          "Composite_var" = "x"
        )
      
      # Compile all information in one df; do not append to list and rbind all
      # entries as R will become unresponsive and require a restart.
      COMD_nonclim_vars_df <-
        rbind(COMD_nonclim_vars_df, COMD_combined_meta)
      
      # Keep track of the last data set opened. This allows for limiting the number
      # of json file accesses
      COMD_last_ds_open <- as.character(COMD_fullID)
      
    } else if (COMD_baseID == 19007)
    {
      # For the climate data, use the metadata from the sensor_description file
      COMD_clim_meta_file <- list.files(getwd(),
                                        pattern = paste0("^", COMD_baseID),
                                        full.names = TRUE)
      COMD_clim_meta_data <-
        read.csv(COMD_clim_meta_file, check.names = FALSE)
      
      # Correct variable names by replacing spaces with underscores
      COMD_clim_meta_data$name <-
        gsub("\\s+", "_", COMD_clim_meta_data$name)
      
      # Subset for variables in FQC composite
      COMD_basic_meta <- as.data.frame(COMD_basic_meta)
      COMD_basic_meta_selVar <- COMD_basic_meta$COMD_trimmed_var
      COMD_clim_meta_data_sub <-
        subset(COMD_clim_meta_data,
               COMD_clim_meta_data$name == COMD_basic_meta_selVar)
      
      if (nrow(COMD_clim_meta_data_sub) == 1) {
        # Compile all metadata
        COMD_clim_combined_meta <-
          cbind(COMD_basic_meta,
                COMD_clim_meta_data_sub,
                COMD_metadata_sub)
        
        # Harmonize column headers with non-climate data
        COMD_clim_combined_meta$BaseID <- NULL
        COMD_clim_combined_meta <- COMD_clim_combined_meta %>%
          rename(
            "Variables.Description" = "description",
            "Variables.unit.Name" = "unit",
            "FullID" = "COMD_fullID",
            "BaseID" = "COMD_baseID",
            "Composite_var" = "x",
            "Variables.Label" = "name",
          )
        
        # Append climate metadata to data frame
        COMD_clim_vars_df <-
          rbind(COMD_clim_vars_df, COMD_clim_combined_meta)
      } else if (nrow(COMD_clim_meta_data_sub) == 0) {
        cat(
          "Warning: The variable",
          COMD_basic_meta_selVar,
          "was not found in the climate metadata. Check your data.\n"
        )
      } else if (nrow(COMD_clim_meta_data_sub) > 1) {
        cat(
          "Warning: Multiple entries were found for the variable",
          COMD_basic_meta_selVar,
          "in the climate metadata. Check your data.\n"
        )
      }
    }
  }
  
  # Combine climate and non-climate metadata
  COMD_all_vars_df <-
    bind_rows(COMD_nonclim_vars_df, COMD_clim_vars_df)
  
  # Combine processing information in one string to be appended to the variables'
  # description
  COMD_metadata_colnames <-
    COMD_metadata_colnames[COMD_metadata_colnames != "BaseID"]
  
  COMD_concat_selcols_function <- function(x) {
    COMD_concat_res <-
      paste(COMD_metadata_colnames, x[names(x) %in% COMD_metadata_colnames], sep = "= ")
    return(COMD_concat_res)
  }
  
  COMD_concat_res_list <-
    apply(COMD_all_vars_df, 1, COMD_concat_selcols_function)
  
  # Reshape back to original data structure and class
  COMD_concat_res_df <-
    as.data.frame(t(as.data.frame(COMD_concat_res_list)))
  
  # Concatenate processing information row-wise and append to compiled metadata
  COMD_concat_rowwise_function <-
    apply(COMD_concat_res_df, 1, function(row)
      paste(row, collapse = ", "))
  COMD_concat_res_total <-
    data.frame(Proc_info = COMD_concat_rowwise_function)
  COMD_all_vars_df <- cbind(COMD_all_vars_df, COMD_concat_res_total)
  
  # Harmonize column names
  COMD_all_vars_df <- COMD_all_vars_df %>%
    rename(
      "Composite_var_trimmed" = "COMD_trimmed_var",
      "Aggr_string_1" = "COMD_aggr_string_1",
      "Aggr_string_2" = "COMD_aggr_string_2",
    )
  
  # Check if the number of compiled metadata entries is equal to the number of
  # columns in FQC composite minus template columns, and export
  if (nrow(COMD_all_vars_df) != (length(FQC_composite) - length(plotIDs_template))) {
    cat(
      "Warning: The FQC composite data set has non-metadata columns that do not appear in the metadata compiled.\n"
    )
  } else {
    cat(
      "Info: The metadata for all non-metadata columns in the FQC composite data set were successfully compiled.\n"
    )
  }
  
  setwd(here("Output"))
  write.csv(COMD_all_vars_df,
            "COMD_metadata_compiled.csv",
            row.names = FALSE)
  
} else if (length(COMD_metadata_files) == 0) {
  cat("Warning: No datastructure files found in 'Metadata'. Skipping ...\n")
} else if (length(COMD_metadata_files) > 0 &
           !(all(unique(datasets_table$BaseID) %in% COMD_metadata_files))) {
  COMD_missing_metadata_files <-
    datasets_table$BaseID[!(unique(datasets_table$BaseID) %in% COMD_metadata_files)]
  cat(
    "Warning: The following data sets are listed in datasets_table but do not have a datastructure file in the Metadata folder. Correct this issue and re-run this step:",
    COMD_missing_metadata_files,
    "\n"
  )
}

# Calculate the time elapsed and prompt about the pipeline's finish
total_runtime <- round(difftime(Sys.time(), start_time, units = "min"), 0)
cat(paste0("\n#########################################################################\nInfo: BEpipeR is done. Total execution time: ", total_runtime, " min. Before moving on, \nplease check all console output generated thoroughly for errors/warnings.\n#########################################################################\n"))

# ------------------------------------------------------------------------------